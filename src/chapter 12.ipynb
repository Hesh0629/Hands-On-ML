{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51ceaad6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:21.244791Z",
     "start_time": "2022-04-28T07:00:21.215564Z"
    }
   },
   "outputs": [],
   "source": [
    "# 파이썬 ≥3.5 필수\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# 사이킷런 ≥0.20 필수\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# 이 노트북은 텐서플로 ≥2.4이 필요합니다\n",
    "# 2.x 버전은 대부분 동일한 결과를 만들지만 몇 가지 버그가 있습니다.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.4\"\n",
    "\n",
    "# 공통 모듈 임포트\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 노트북 실행 결과를 동일하게 유지하기 위해\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 깔끔한 그래프 출력을 위해\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e776d2",
   "metadata": {},
   "source": [
    "# 텐서플로를 사용한 사용자 정의 모델과 훈련\n",
    "케라스를 넘어 이제는 텐서플로 그자체이다.  \n",
    "구글이 개발한 수치 계산을 위한 라이브러리이기에 대규모 머신러닝에 잘 맞도로 튜닝되어 있다.  \n",
    "핵심구조는 Numpy와 유사하지만 GPU를 지원하기에 병렬연산이 가능하고 분산 컴퓨팅을 지원한다.  \n",
    "C++ 코드로 구현되어 있고, 많은 연산들이 커널이라고 부르는 여러 구현을 가진다.  \n",
    "## 텐서플로를 Numpy 처럼 이용하기\n",
    "텐서플로 API는 텐서라는 넘파이의 ndarray와 비슷한 다차원 배열의 자료구조를 순환시킨다.  \n",
    "텐서는 한 연산에서 다른 연산으로 흘러다니고 이 때문에 tensorFlow라고 부른다\n",
    "### 텐서와 연산\n",
    "tf.constant() 함수로 텐서를 만들 수 있다. 사실 Numpy랑 엄청 닮았다. (라고 하기엔 numpy속성을 가진다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d3f626e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:21.260211Z",
     "start_time": "2022-04-28T07:00:21.246108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32) \n",
      "\n",
      "(2, 3) \n",
      "\n",
      "tf.Tensor(42, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "print(t, '\\n')  # 행렬\n",
    "print(t.shape, '\\n')  # tf.Tensor는 shape와 dtype을 속성으로 가진다.\n",
    "print(tf.constant(42))  # 스칼라 값도 가능!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded863f3",
   "metadata": {},
   "source": [
    "인덱스 참조나 연산 함수도 Numpy하고 매우 비슷하게 작동한다. (물론 기억이 가물가물)  \n",
    "그런데 종종 다른 것도 있으니 해보고 안되면 API 레퍼런스 찾아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24c255fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:21.275369Z",
     "start_time": "2022-04-28T07:00:21.261217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2. 3.]\n",
      " [5. 6.]], shape=(2, 2), dtype=float32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[2.]\n",
      " [5.]], shape=(2, 1), dtype=float32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[11. 12. 13.]\n",
      " [14. 15. 16.]], shape=(2, 3), dtype=float32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[ 1.  4.  9.]\n",
      " [16. 25. 36.]], shape=(2, 3), dtype=float32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[14. 32.]\n",
      " [32. 77.]], shape=(2, 2), dtype=float32) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(t[:, 1:], '\\n')\n",
    "\n",
    "# tf.newaxis는 차원을 추가시켜 준다. 원래대로면 shape가 (2)였겠지만 (2,1)로 만든다.\n",
    "print(t[:, 1, tf.newaxis], '\\n')\n",
    "\n",
    "print(t + 10, '\\n')\n",
    "print(tf.square(t), '\\n')\n",
    "\n",
    "# @ 기호는 행렬 곱셈을 의미한다 == tf.matmul()\n",
    "# t.T 와 같이 numpy에서는 가능했지만 텐서플로는 불가능하다;;\n",
    "print(t @ tf.transpose(t), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab89de66",
   "metadata": {},
   "source": [
    "### 텐서와 Numpy\n",
    "텐서는 Numpy와 함께 사용하기 좋다. 텐서로 넘파이 배열을 만들 수 있고 그 역도 된다.  \n",
    "텐서에 Numpy연산이 되고 그 역도 된다! 다만, 넘파이는 64bit 정밀도를, 텐서플로는 32bit 정밀도를 기본값으로 이용하기에  \n",
    "넘파이 배열로 텐서를 만들 때 dtype=tf.float32 로 지정해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "04c24716",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:21.290608Z",
     "start_time": "2022-04-28T07:00:21.276450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2. 4. 5.], shape=(3,), dtype=float64) \n",
      "\n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32) \n",
      "\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "tf.Tensor([ 4. 16. 25.], shape=(3,), dtype=float64) \n",
      "\n",
      "[[ 1.  4.  9.]\n",
      " [16. 25. 36.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([2., 4., 5.])\n",
    "print(tf.constant(a), '\\n') # Numpy 배열로 텐서 만들기\n",
    "\n",
    "print(t, '\\n')   # 텐서\n",
    "print(t.numpy()) # Numpy 배열 (np.array(t) 와 같다)\n",
    "\n",
    "print(tf.square(a),'\\n') # 텐서 연산을 넘파이 배열에 적용\n",
    "print(np.square(t))      # 넘파이 연산을 텐서에 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4a6df7",
   "metadata": {},
   "source": [
    "### 타입 변환\n",
    "타입이 자동으로 바뀌다보면 성능이 저하되고 사용자가 눈치채지 못한다. 그래서 텐서플로는 타입 변환을 안시켜준다.  \n",
    "문제는 그래서 타입이 서로 호환되지 않으면 이용할 수 가 없다. 32bit 실수형과 64bit 실수형도 짤없다.\n",
    "```python\n",
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)\n",
    "```\n",
    "### 변수\n",
    "tf.constant(Tensor) 대신에 tf.Variable로 선언해주면 값을 변경하는 것이 가능하다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72d21d5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:21.306421Z",
     "start_time": "2022-04-28T07:00:21.292250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6fd3d",
   "metadata": {},
   "source": [
    "(그렇지 않으면 역전파로 갱신해야 하는 신경망의 가중치를 조작하는 것이 불가능)  \n",
    "tf.Variable 은 tf.Tensor와 비슷하게 동작하고 연산도 비슷하다.  \n",
    "다만 assign() 메서드를 사용하여 변숫값을 변경할 수 있다.  \n",
    "scatter_update(), scatter_nd_update() 메서드를 사용하면 개별원소 수정 및 슬라이스를 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b8fadee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:21.322199Z",
     "start_time": "2022-04-28T07:00:21.307240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 원소에 2를 곱한다.\n",
    "v.assign(2*v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3fdd27b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:21.338214Z",
     "start_time": "2022-04-28T07:00:21.323197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v[0][1] 위치에 42를 넣어준다.\n",
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc489b63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:21.354336Z",
     "start_time": "2022-04-28T07:00:21.339198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  0.],\n",
       "       [ 8., 10.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2번째 열에 각각 0. 1. 을 넣어준다.\n",
    "v[:,2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d45d0bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:21.369460Z",
     "start_time": "2022-04-28T07:00:21.355368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v[0][0]와 v[1][2]에 각각 값을 넣어줌\n",
    "v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad81f153",
   "metadata": {},
   "source": [
    "## 사용자 정의 모델과 훈련 알고리즘\n",
    "필요한 함수가 라이브러리에 구현되어 있지 않다면? 직접 구현하자!\n",
    "### 사용자 정의 손실 함수\n",
    "회귀 모델에서 MSE는 큰 오차에 과한 패너티를, MAE는 이상치에 관대해서 수렴되는 데 오래걸린다.  \n",
    "이 때 이용하는 것이 ***후버 손실*** 이다.\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSwD2L7a4-lvXvVy5YBhp4m7nXKszRWedSLrQ&usqp=CAU\">  \n",
    "tf.keras에 구현되어 있지만 공식 케라스 API에 있진 않다. 그러면 직접 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d3d26efb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:21.400562Z",
     "start_time": "2022-04-28T07:00:21.370591Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 텐서플로의 장점을 활용하려면 텐서플로 연산만을 이용하자.\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss  = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe001893",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.434928Z",
     "start_time": "2022-04-28T07:00:21.401734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 987us/step - loss: 0.6235 - mae: 0.9953 - val_loss: 0.2862 - val_mae: 0.5866\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 771us/step - loss: 0.2197 - mae: 0.5177 - val_loss: 0.2382 - val_mae: 0.5281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1df3de92040>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,\n",
    "                       activation=\"selu\",\n",
    "                       kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled,\n",
    "          y_train,\n",
    "          epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d4935e",
   "metadata": {},
   "source": [
    "### 사용자 정의 요소를 가진 모델을 저장하고 로드하기\n",
    "만약 위의 huber_fn 처럼 사용자 정의 손실함수 처럼 사용자 정의 요소를 사용하더라도 모델의 저장은 잘 이뤄진다.\n",
    "<br>\n",
    "다만, 모델을 로드할 때, 사용자 정의 요소의 이름과 객체를 매핑한 딕셔너리를 전달해야 한다.\n",
    "```python\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n",
    "                                custom_objects={\"huber_fn\": huber_fn})\n",
    "```\n",
    "만약 후버 손실 함수에서 오차의 기준을 바꾸고 싶어서 파라미터를 받을 수 있도록 한다고 하자.\n",
    "```python\n",
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "```\n",
    "그러고 모델을 저장한다면 threshold가 저장되기를 원하지만 실제로는 그렇지 않다.  \n",
    "따라서 다음과 같이 모델을 로드할 때, 딕셔너리에 넘겨줘야 한다.\n",
    "```python\n",
    "keras.models.load_model(\n",
    "    \"my_model_with_a_custom_loss2.h5\",\n",
    "    custom_objects={\"huber_fn\": create_huber(2.0)}\n",
    ")\n",
    "```\n",
    "만약 이렇게 매번 지정해주기 싫다면 keras.losses.Loss 클래스를 상속하고 get_config()를 구현하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a10ae240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.450298Z",
     "start_time": "2022-04-28T07:00:22.436114Z"
    }
   },
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs) # super() : 부모 클래스를 호출해준다.\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config() # 부모 클래스에서 get_config()로 정보를 받아온다.\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "        # **는 딕셔너리 언패킹 연산자로 이를 통해 딕셔너리를 파라미터로 빠르게 넘겨줄 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fbc85",
   "metadata": {},
   "source": [
    "이런식으로 손실함수를 선언해주면 나중에 모델을 컴파일할 때 임계값도 같이 넘어가고 저장된다.   \n",
    "모델을 저장할 때 케라스가 get_config() 메서드를 호출해서 반환된 설정을 HDF5 파일에 JSON형태로 저장해준다.\n",
    "<br>\n",
    "모델을 로드하면 HuberLoss 클래스의 from_config() 클래스 메서드를 호출하는데  \n",
    "이 메서드는 기본 손실 클래스  (Loss)에 구현되어 있으며  \n",
    "생성자에게 ******config 파라미터를 전달해서 클래스의 인스턴스를 만든다.\n",
    "```python\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.save(\"my_model_with_a_custom_loss_class.h5\")\n",
    "model = \n",
    "    keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\",\n",
    "                             custom_objects={\"HuberLoss\": HuberLoss})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42336345",
   "metadata": {},
   "source": [
    "### 활성화 함수, 초기화, 규제, 제한을 커스터마이징하기\n",
    "대부분의 케라스 기능들은 유사한 방법으로 커스터마이징이 가능하다.  \n",
    "대부분의 입력과 출력을 가진 간단한 함수를 작성하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac73cb9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.465452Z",
     "start_time": "2022-04-28T07:00:22.451306Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_softplus(z):  # tf.nn.softplus(z) 값을 반환합니다\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "\n",
    "def my_positive_weights(weights):  # tf.nn.relu(weights) 값을 반환합니다\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n",
    "\n",
    "layer = keras.layers.Dense(1,\n",
    "                           activation=my_softplus,\n",
    "                           kernel_initializer=my_glorot_initializer,\n",
    "                           kernel_regularizer=my_l1_regularizer,\n",
    "                           kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d5d745",
   "metadata": {},
   "source": [
    "함수가 모델과 함께 저장해야 하는 하이퍼파라미터를 가지고 있다면 다음과 같은 클래스를 상속하자.\n",
    "```python\n",
    "keras.regularizers.Regularizer\n",
    "keras.constraints.Constraint\n",
    "keras.initializers.Initializer\n",
    "keras.layers.Layer\n",
    "```\n",
    "그 다음부터는 사용자 정의 손실을 만들었던 것처럼 init, call, get_config를 만들어주자.\n",
    "```python\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        \n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}\n",
    "```\n",
    "다만 사용자 정의 지표 (Metrics)는 조금 다르다.\n",
    "### 사용자 정의 지표\n",
    "손실과 지표가 아주 다른 것은 아니지만 지표는 미분 불능이어도 되고, 보기좋고 이해하기 편해야한다.  \n",
    "그렇기에 사용자 정의 지표를 사용해서 내가 보기 쉽게 만들어보자.  \n",
    "대부분의 경우 지표를 만드는 방식은 손실 함수를 만드는 것과 동일하다.  \n",
    "당장 후버 손실 함수를 지표로 가져다 써도 잘 작동한다.\n",
    "```python\n",
    "model.compile.(loss=\"mse\", \n",
    "               optimizer=\"nadam\", \n",
    "               metrics=[create_huber(2.0)])\n",
    "```\n",
    "훈련하는 동안 각 배치에 대하여 케라스는 지표를 계산하고 에포크마다 시작할 때부터 평균을 새롭게 계산한다.  \n",
    "이 방식이 왠만하면 맞는 편이지만 항상 맞지는 않는다. 당장 정밀도 (precision)을 예시로 들어보자.  \n",
    "우리는 정밀도를 내가 양성이라고 판정내린 것들중에 실제 양성인것의 비율이라고 부른다.  \n",
    "만약 매번 에포크마다 계산을하고 평균을 낸다면 모델 전체의 정밀도를 구할 수 없다. 따라서 점진적으로 전체 정밀도를\n",
    "<br>업데이트해줘야 하는데 이를 위해 keras.metrics.Precision 클래스를 사용한다.  \n",
    "이 클래스는 배치를 처리하면서 지금까지 전체 정밀도를 계산해준다.  \n",
    "따라서 점진적으로 업데이트되므로 스트리밍 지표 또는 stateful metrics 라고 부른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "772b0081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.480592Z",
     "start_time": "2022-04-28T07:00:22.466465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "130bf098",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.495760Z",
     "start_time": "2022-04-28T07:00:22.481652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 지표를 계속해서 업데이트 해준다.\n",
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bc1d0b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.510991Z",
     "start_time": "2022-04-28T07:00:22.496788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중간 결과를 보고싶다면 result() 메서드를 이용하자.\n",
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bc201e08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.526145Z",
     "start_time": "2022-04-28T07:00:22.512039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  중간 true pos 와 false pos를 저장한 variables attribs\n",
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aef430d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.541486Z",
     "start_time": "2022-04-28T07:00:22.527224Z"
    }
   },
   "outputs": [],
   "source": [
    "precision.reset_states() # 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144e342",
   "metadata": {},
   "source": [
    "만약 이런 스트리밍 지표를 만들고 싶다면 keras.metrics.Metric 클래스를 상속한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "04f682f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.557664Z",
     "start_time": "2022-04-28T07:00:22.542506Z"
    }
   },
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # 기본 매개변수 처리 (예를 들면, dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "        \n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b1dae",
   "metadata": {},
   "source": [
    "### 사용자 정의 층 (layer)\n",
    "텐서플로에 없는 충이나, 반복되는 여러 층을 묶어서 사용자 정의 layer로 만들 수 있다.  \n",
    "먼저, 가중치가 필요 없는 사용자 정의 층을 만들기 위해 keras.layers.Lambda 층으로 감싸자.  \n",
    "```python\n",
    "exponential_layer = keras.layers.Lambda(lambda x:tf.exp(x))\n",
    "```\n",
    "여담이지만 지수 함수는 이따금 회귀 모델에서 예측값의 스케일이 매우 다를 때 이용할 수 있다.  \n",
    "<br>\n",
    "상태가 있는 층, 즉 가중치를 가진 층을 만들기 위해서는 keras.layers.Layer 를 상속해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "26df4322",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.572853Z",
     "start_time": "2022-04-28T07:00:22.558746Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        #문자열로 받아서 활성화 함수 객체를 돌려준다.\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\",\n",
    "            shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(name=\"bias\",\n",
    "                                    shape=[self.units],\n",
    "                                    initializer=\"zeros\")\n",
    "        \n",
    "        # must be at the end, 그래야 케라스가 층이 만들어 졌음을 알 수 있다.\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        # 이 층의 출력 크기를 반환한다. 이 예시에서는 마지막 차원을 제외하고 입력과 크기가 같다.\n",
    "        # 참고로 마지막 차원의 크기는 이 층의 뉴런 개수이다.\n",
    "        # 케라스에서 크기는 tf.TensorShape 객체로 받아올 수 있다.\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config, \n",
    "            \"units\": self.units,\n",
    "            # keras.activations.serialize()를 이용하여 활성화 함수의 모든 설정을 저장한다.\n",
    "            \"activation\": keras.activations.serialize(self.activation)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7aed2d8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.589099Z",
     "start_time": "2022-04-28T07:00:22.573864Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyNormalization(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 키워드 arg는 **를 붙여서 함수에 파라미터로 보내주면 = 이 붙어서 나간다.\n",
    "    def build(self, batch_input_shape):\n",
    "        self.alpha = self.add_weight(name=\"alpha\",\n",
    "                                     shape=[batch_input_shape[-1]],\n",
    "                                     initializer=\"RandomUniform\")\n",
    "        self.beta = self.add_weight(name=\"beta\",\n",
    "                                    shape=[batch_input_shape[-1]],\n",
    "                                    initializer=\"RandomUniform\")\n",
    "        # must be at the end, 그래야 케라스가 층이 만들어 졌음을 알 수 있다.\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, X):\n",
    "        mean, var = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "        std = var**0.5\n",
    "        return self.alpha * (X - mean) / (std + 0.001) + self.beta\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"alpha\": self.alpha, \"beta\": self.beta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b0c36744",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.604376Z",
     "start_time": "2022-04-28T07:00:22.590211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.6674843>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_train.astype(np.float32)\n",
    "\n",
    "custom_layer_norm = MyNormalization()\n",
    "keras_layer_norm = keras.layers.LayerNormalization()\n",
    "\n",
    "tf.reduce_mean(keras.losses.mean_absolute_error(\n",
    "    keras_layer_norm(X), custom_layer_norm(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87e76a2",
   "metadata": {},
   "source": [
    "여러가지 입력을 받는 layer (예를 들어 concat layer)를 만들 때는 call() 메서드에  \n",
    "모든 입력이 포함된 튜플을 파라미터로 넘겨줘야 한다.  \n",
    "이와 마찬가지로 compute_output_shape() 를 만들때도 각 입력의 배치 크기를 담은 튜플을 넘겨줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d0f371f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.619507Z",
     "start_time": "2022-04-28T07:00:22.605396Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        print(\"X1.shape: \", X1.shape, \" X2.shape: \", X2.shape)  # 사용자 정의 층 디버깅\n",
    "        return X1 + X2, X1 * X2\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        batch_input_shape1, batch_input_shape2 = batch_input_shape\n",
    "        return [batch_input_shape1, batch_input_shape2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e99f394",
   "metadata": {},
   "source": [
    "훈련과 테스트에서 다르게 동작하는 드롭아웃이나 배치 정규화층과 같은 layer라면 call() 메서드에  \n",
    "training 파라미터를 추가하여 훈련인지 테스트인지 결정하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ccbc7d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.634843Z",
     "start_time": "2022-04-28T07:00:22.620719Z"
    }
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04872dd",
   "metadata": {},
   "source": [
    "### 사용자 정의 모델\n",
    "keras.Model 클래스를 상속하여 생성자에서 층과 변수를 만들자.  \n",
    "스킵연결이 있는 잔차 블록을 3번 반복 시키고 다음 잔차 블록으로 넘기는 모델을 만들어보자.  \n",
    "우선 출력값에 입력값을 더해주는 잔차 블록 (Residual block) 을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5b795b3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.650409Z",
     "start_time": "2022-04-28T07:00:22.636388Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [\n",
    "            keras.layers.Dense(n_neurons,\n",
    "                               activation=\"elu\",\n",
    "                               kernel_initializer=\"he_normal\")\n",
    "            for _ in range(n_layers) # n_layer 만큼 레이어의 개수를 늘려준다.\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c2620afb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.666585Z",
     "start_time": "2022-04-28T07:00:22.651417Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83076e2a",
   "metadata": {},
   "source": [
    "save() 메서드와 load_model() 을 사용하고 싶다면 layer와 model 클래스 모두 get_config()를 구현하자.\n",
    "<br>\n",
    "또한 이 과정에서 weights가 저장되진 않으니 save_weights() 와 load_weights() 를 사용하면 된다.  \n",
    "### 모델 구성 요소에 기반한 손실과 지표\n",
    "앞선 사용자 손실과 지표 모두 예측값과 실제 레이블을 기반으로 한다.  \n",
    "하지만 때때로 은닉층의 가중치나 활성화 함수와 같이 모델의 구성요소에 기반한 손실을 정의해야 할 때가 있다.  \n",
    "이런 손실은 규제나 모델의 내부 상황을 모니터링할 때 유용하다.  \n",
    "<br>\n",
    "연습을 위하여 맨 위 은닉층에 보조 출력으로 재구성 손실을 구한다. ***(재구성과 입력 사이의 MSE)***  \n",
    "재구성 손실을 주 손실에 더하여 회귀에 도움이 안되도 은닉층을 통과하면서도 가능한 많은 정보를 유지하도록 해보자\n",
    "<br>\n",
    "(사실 이래야 규제 손실처럼 동작하며 일반화 성능이 강화된다)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b6e82c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.681655Z",
     "start_time": "2022-04-28T07:00:22.667672Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [\n",
    "            keras.layers.Dense(30,\n",
    "                               activation=\"selu\",\n",
    "                               kernel_initializer=\"lecun_normal\")\n",
    "            for _ in range(5)\n",
    "        ]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "\n",
    "        # *중요* layer를 상속받고 build 메서드를 작성할 때는 super().build()를 해주는게 맞다.\n",
    "        # 그런데 TF 2.2 에서 발생한 이슈로 model을 상속받고 build 메서드를 작성할 때는 쓰면 안된다.\n",
    "        # super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "\n",
    "        # 훈련중에 재구성과 입력 사이에 발생한 제곱 오차를 모델의 손실 리스트에 추가한다.\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646909d5",
   "metadata": {},
   "source": [
    "### 자동 미분을 사용하여 그레디언트 계산하기\n",
    "신경망은 워낙 커서 도함수를 계산하기가 어렵다. 파라미터 마다 계산하기도 어렵기에 자동 미분을 사용한다.  \n",
    "이를 위해 tf.GradientTape() 를 만들어 이 변수와 관련된 모든 연산을 기록하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e24be785",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.697766Z",
     "start_time": "2022-04-28T07:00:22.683656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1**2 + 2 * w1 * w2\n",
    "\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "# 이러면 w1, w2에 대한 z의 그레디언트가 저장된다.\n",
    "# 다만, gradient()를 한번이라도 호출하는 순간 자동으로 테이프가 지워지기에 2번 이상 호출하면 에러가 발생\n",
    "# 지속 가능한 테이프를 만들고 싶다면 tf.GradientTape(persistent=True)로 선언하자.\n",
    "gradients = tape.gradient(z,[w1,w2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c04ce25",
   "metadata": {},
   "source": [
    "테이프는 변수에 대해서만 연산을 기록해준다. 만약 tf.Constant를 이용한다면 None이 반환된다.  \n",
    "굳이 Constant에 대해서도 보고싶다면 테이프 내부에서 tape.watch(모든 텐서) 로 써서 강제할 수 있다.  \n",
    "입력값 처럼 변수가 아닌 값들에 대해 변동 폭이 큰 활성화 함수에 대한 규제 손실을 구현할 때 유용하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "06a4f207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.712597Z",
     "start_time": "2022-04-28T07:00:22.699300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b7308",
   "metadata": {},
   "source": [
    "가끔은 그레디언트가 역전파되지 않도록 막을 필요가 있다. 이를 위해 tf.stop_gradient()를 이용하자.  \n",
    "그러면 f(w1, w2)를 계산할 때 (정방향)는 값이 제대로 나오지만 미분값을 계산할 때는 상수항 취급된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bf3cf18b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.727766Z",
     "start_time": "2022-04-28T07:00:22.713704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecfb992",
   "metadata": {},
   "source": [
    "자동 미분을 사용하여 함수의 그레디언트를 계산하는 것은 수치적으로 불안정하다.  \n",
    "부동소수점 정밀도 오차에 의해 무한 나누기 무한을 계산할 수 있다. (NaN반환)  \n",
    "예를 들어 my_softplus() 함수의 그레디언트를 구한다 했을 때 자동미분을 이용하면 NaN이 반환된다.  \n",
    "이를 막기위해 일반 출력과 도함수를 계산하는 함수를 반환해서 안전한 계산을 유도하자  \n",
    "```python\n",
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        # 원래는 exp (1 + 1 / exp) 이다.\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89368b1",
   "metadata": {},
   "source": [
    "### 사용자 정의 훈련 반복\n",
    "fit() 마저도 유연하지 않을 수 있다. 예를 들면 두 개의 다른 옵티마이저를 사용한다거나...  \n",
    "이 경우 fit()은 하나의 옵티마이저만 이용하므로 불가능하다.  \n",
    "이를 위해 사용자 정의 훈련을 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f15b64fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:22.743901Z",
     "start_time": "2022-04-28T07:00:22.728791Z"
    }
   },
   "outputs": [],
   "source": [
    "# 우선 모델을 정의하자.\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,\n",
    "                       activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])\n",
    "\n",
    "# 샘플 배치를 랜덤하게 추출하는 함수\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "# 메트릭스 이름과 결과를 출력한다.\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\n",
    "        \"{}: {:.4f}\".format(m.name, m.result())\n",
    "        for m in [loss] + (metrics or [])\n",
    "    ])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c9afc956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:33.561308Z",
     "start_time": "2022-04-28T07:00:22.745528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11610/11610 - mean: 1.4513 - mean_absolute_error: 0.5865\n",
      "Epoch 2/5\n",
      "11610/11610 - mean: 0.6700 - mean_absolute_error: 0.5286\n",
      "Epoch 3/5\n",
      "11610/11610 - mean: 0.6346 - mean_absolute_error: 0.5192\n",
      "Epoch 4/5\n",
      "11610/11610 - mean: 0.6422 - mean_absolute_error: 0.5212\n",
      "Epoch 5/5\n",
      "11610/11610 - mean: 0.6454 - mean_absolute_error: 0.5246\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        \n",
    "        # 훈련가능한 모든 변수에 대한 손실함수의 그레디언트를 계산한다.\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            \n",
    "            # mean_squared_error 가 하나의 샘플에 대한 값을 반환하므로\n",
    "            # reduce_mean 을 이용하여 배치에 대한 평균을 구해주자.\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            \n",
    "            # 층마다 존재하는 규제 손실 (model.losses)를 더해주자.\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        # 모델에 가중치 제한이 추가되면 변수들에 대하여서 constraint를 적용해주자.\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        \n",
    "        mean_loss(loss) # 스트리밍 지표임 (지금까지의 평균 오차 계산)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca638b3d",
   "metadata": {},
   "source": [
    "책에서도 언급했듯이 주의할 점도 많고 복잡하다. 하지만 확실한 것은 완전하게 모델을 제어할 수 있다는 장점이 있다.\n",
    "## 텐서플로 함수와 그래프\n",
    "텐서플로에서 그래프는 API의 핵심이므로 피할 수 없었다. 예전이야 복잡했지만 지금은 쉬워졌다.  \n",
    "예시로 파이썬 함수를 만들면 정수나 파이썬 상수, 텐서를 통해 호출 할 수 있다.  \n",
    "텐서플로 함수는 불필요한 노드를 제거하고 표현을 단순화 시켜주는 식으로 계산 그래프를 최적화시킨다.  \n",
    "따라서 원본 파이썬 함수보다 훨씬 빠르게 실행되므로 복잡한 연산을 수행할 때 필수적이다.  \n",
    "그런데 사용자 정의 함수를 작성하고 케라스 모델에 가져다 쓰면 알아서 케라스가 텐서플로 함수로 변환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0cbd6ee2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:33.576797Z",
     "start_time": "2022-04-28T07:00:33.562440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "print(cube(2))\n",
    "print(cube(tf.constant(2.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2a67f",
   "metadata": {},
   "source": [
    "이 파이썬 함수를 이제 tf.function() 을 사용하여 텐서플로 함수로 만들면   \n",
    "파이썬 함수로 이용하는 것은 당연하고 동일한 결과를 텐서 형태로 반환해준다.  \n",
    "내부적으로는 파이썬 함수에서 수행되는 계산을 분석하고 동일한 작업을 수행하는 계산 그래프를 생성한다.  \n",
    "다만, 같은 크기의 자료형을 가지는 입력값이 들어오면 그래프를 재활용해주고 새로운 유형이면 새롭게 그래프를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a67c5b6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:33.592052Z",
     "start_time": "2022-04-28T07:00:33.577932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf_cube = tf.function(cube)\n",
    "print(tf_cube(2))\n",
    "print(tf_cube(tf.Variable(2.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b92cb2",
   "metadata": {},
   "source": [
    "다른방법으로는 tf.function 데코레이터가 더 널리 사용된다.  \n",
    "저렇게 @tf.function 쓰고 밑에다가 사용하고 싶은 함수를 작성하면 알아서 텐서플로 함수로 바꿔준다.\n",
    "```python\n",
    "@tf.function\n",
    "def tf_tube(x):\n",
    "    return x ** 3\n",
    "```\n",
    "또한 원본 파이썬 함수는 python_function 속성으로 참조할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b9061978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T07:00:33.607218Z",
     "start_time": "2022-04-28T07:00:33.593112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.cube(x)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube.python_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b441b40",
   "metadata": {},
   "source": [
    "### 오토그래프와 트레이싱\n",
    "텐서플로는 파이썬 함수의 소스코드를 분석하여 모든 제어문들을 텐서플로 연산으로 바꿔준다.  \n",
    "이를 ***오토그래프*** 라고 부르며 이를 통해 업그레이드된 함수를 작성하게 된다.  \n",
    "그다음, 이 함수를 파라미터 대신 ***심볼릭 텐서*** 라는 이름의 값이 없지만 이름, 데이터 타입과 크기를 가진 텐서를 넘겨줘서  \n",
    "호출시킨다. 이 함수는 ***그래프 모드*** 로 실행되며 각 텐서플로 연산들은 계산을 수행하는 것 대신에,  \n",
    "텐서를 출력하기 위한 그래프 노드를 추가시켜준다.\n",
    "### 텐서플로 함수 주의점\n",
    "1. 넘파이나 표준 라이브러리를 포함한 텐서플로 이외의 라이브러리를 사용하면 트레이싱 과정중에 실행된다.  \n",
    "심지어 그래프에 포함도 되지 않기 때문에 텐서플로 구성 요소로 작성하자.  \n",
    "2. 다른 파이썬 함수나 텐서플로 함수를 호출할 수 있다. 이 경우 해당 함수들에 대해서 데코레이터를 안써줘도 된다.\n",
    "<br>\n",
    "3. 함수에서 텐서플로 변수를 만든다면 처음 호출될 때만 수행되어야 한다.\n",
    "4. 파이썬 함수의 소스코드가 텐서플로에서 이용 가능해야 한다.\n",
    "5. for문에서 텐서나 데이터셋을 순회하는 for문만 써야한다. (for i in range(x) 이용 불가능)\n",
    "6. 성능면에서 반복문보다 벡터화된 구현이 더 좋다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.458px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
