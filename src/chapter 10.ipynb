{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe5614f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:27:51.251785Z",
     "start_time": "2022-04-18T04:27:42.824222Z"
    }
   },
   "outputs": [],
   "source": [
    "# 파이썬 ≥3.5 필수\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# 사이킷런 ≥0.20 필수\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# 텐서플로 ≥2.0 필수\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# 공통 모듈 임포트\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 노트북 실행 결과를 동일하게 유지하기 위해\n",
    "np.random.seed(42)\n",
    "\n",
    "# 깔끔한 그래프 출력을 위해\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('default')\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d352e",
   "metadata": {},
   "source": [
    "# 케라스를 사용한 인공 신경망\n",
    "드디어 신경망으로 넘어왔다.  \n",
    "다층 퍼셉트론을 이용하여 유연하고 높은 표현력을 가진 신경망을 케라스로 만들자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f671aa77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:27:51.517338Z",
     "start_time": "2022-04-18T04:27:51.252763Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79cd91d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:27:51.533339Z",
     "start_time": "2022-04-18T04:27:51.518337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "print(X_train_full.shape)\n",
    "print(X_train_full.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c974844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:27:51.657366Z",
     "start_time": "2022-04-18T04:27:51.534338Z"
    }
   },
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9701bb59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:27:51.673353Z",
     "start_time": "2022-04-18T04:27:51.658338Z"
    }
   },
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\",\n",
    "    \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7c33b",
   "metadata": {},
   "source": [
    "## 시퀸셜 API를 이용한 분류 모델 생성\n",
    "keras.models.Sequential()을 이용하여 순서대로 층을 연결하여 모델을 만들어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be7f0c1",
   "metadata": {},
   "source": [
    "###  모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea53f164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:27:51.795339Z",
     "start_time": "2022-04-18T04:27:51.674337Z"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "# 입력 이미지를 reshape해서 1D로 변환\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "# N개의 뉴런을 가진 Dense층을 추가, 활성화 함수는 ReLU 함수.\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "\n",
    "# 마지막 출력을 위한 10개의 뉴런을 가진 Dense층. softmax 활성화를 통해 클래스 별 확률을 출력.\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fefc0b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:27:51.826337Z",
     "start_time": "2022-04-18T04:27:51.796339Z"
    }
   },
   "outputs": [],
   "source": [
    "# 한번에 리스트를 넘기는 것도 가능하다.\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7261f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:27:51.842338Z",
     "start_time": "2022-04-18T04:27:51.827337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.flatten.Flatten at 0x23b62b3d760>,\n",
       " <keras.layers.core.dense.Dense at 0x23b62b3d730>,\n",
       " <keras.layers.core.dense.Dense at 0x23b62b35550>,\n",
       " <keras.layers.core.dense.Dense at 0x23b62b35280>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary()와 layers attribs를 이용하여 정보를 얻어낼 수 있다.\n",
    "\n",
    "print(model.summary())\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30a4fe02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:27:51.857340Z",
     "start_time": "2022-04-18T04:27:51.844338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "print(hidden1.name)\n",
    "model.get_layer('dense_3') is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac9f8461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:27:51.873354Z",
     "start_time": "2022-04-18T04:27:51.858338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04447937 -0.04099248 -0.0193208  ... -0.01441845 -0.03774612\n",
      "  -0.0159855 ]\n",
      " [-0.03694651 -0.03802623  0.01243072 ... -0.05406223 -0.06578163\n",
      "   0.0449766 ]\n",
      " [ 0.03936975  0.03783122  0.05599502 ...  0.04105379  0.04598705\n",
      "  -0.04587892]\n",
      " ...\n",
      " [-0.05520768  0.05928649  0.03959306 ...  0.03314824 -0.06225937\n",
      "  -0.04303161]\n",
      " [-0.06377283  0.06684908  0.02799835 ... -0.04380647 -0.01737352\n",
      "  -0.04024508]\n",
      " [ 0.00231768 -0.01694362 -0.04072764 ... -0.02322719  0.01753905\n",
      "   0.04779348]]\n",
      "(784, 300)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "print(weights)\n",
    "print(weights.shape)\n",
    "print(biases)\n",
    "print(biases.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effcda72",
   "metadata": {},
   "source": [
    "레이블이 정수로 되어있고 배타적이므로 ***sparse_categorical_crossentropy*** 를 손실함수로 이용  \n",
    "단, 원-핫 벡터와 같이 샘플마다 클래스 별 타깃 확률을 가지고 있다면 ***categorical_crossentropy*** 를 이용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "217924c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:27:51.888353Z",
     "start_time": "2022-04-18T04:27:51.874340Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1efc6",
   "metadata": {},
   "source": [
    "### 모델 훈련 및 학습 곡선 확인\n",
    "fit() 메서드가 반환하는 History 객체에는 훈련 파라미터와 에포크 리스트가 담겨 있다.  \n",
    "특히 훈련셋과 검증셋에 대한 손실과 측정지표가 담긴 딕셔너리가 중요한데 이를 통해 학습 곡선을 손쉽게 제작가능하다.\n",
    "<br>\n",
    "validation_data 대신 validation_split을 설정해주면 훈련셋의 마지막 일부를 검증셋으로 이용한다.  \n",
    "class_weight와 sample_weight를 통해 편향된 데이터에 대하여 가중치를 조정 해줄 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c499aba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:41.318338Z",
     "start_time": "2022-04-18T04:27:51.889337Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.7195 - accuracy: 0.7618 - val_loss: 0.4994 - val_accuracy: 0.8344\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 2s 987us/step - loss: 0.4850 - accuracy: 0.8312 - val_loss: 0.4769 - val_accuracy: 0.8366\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 2s 949us/step - loss: 0.4420 - accuracy: 0.8445 - val_loss: 0.4303 - val_accuracy: 0.8520\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 2s 936us/step - loss: 0.4168 - accuracy: 0.8547 - val_loss: 0.3994 - val_accuracy: 0.8696\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 2s 934us/step - loss: 0.3978 - accuracy: 0.8604 - val_loss: 0.3840 - val_accuracy: 0.8702\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 2s 942us/step - loss: 0.3817 - accuracy: 0.8663 - val_loss: 0.3838 - val_accuracy: 0.8714\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 2s 940us/step - loss: 0.3691 - accuracy: 0.8703 - val_loss: 0.3764 - val_accuracy: 0.8708\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 2s 940us/step - loss: 0.3565 - accuracy: 0.8735 - val_loss: 0.3618 - val_accuracy: 0.8740\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 2s 946us/step - loss: 0.3457 - accuracy: 0.8772 - val_loss: 0.3547 - val_accuracy: 0.8766\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 2s 943us/step - loss: 0.3369 - accuracy: 0.8803 - val_loss: 0.3464 - val_accuracy: 0.8792\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 2s 944us/step - loss: 0.3285 - accuracy: 0.8826 - val_loss: 0.3381 - val_accuracy: 0.8834\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 2s 939us/step - loss: 0.3206 - accuracy: 0.8856 - val_loss: 0.3512 - val_accuracy: 0.8740\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 2s 943us/step - loss: 0.3139 - accuracy: 0.8873 - val_loss: 0.3261 - val_accuracy: 0.8854\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 2s 947us/step - loss: 0.3053 - accuracy: 0.8904 - val_loss: 0.3246 - val_accuracy: 0.8840\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 2s 943us/step - loss: 0.2998 - accuracy: 0.8920 - val_loss: 0.3302 - val_accuracy: 0.8794\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 2s 944us/step - loss: 0.2937 - accuracy: 0.8943 - val_loss: 0.3298 - val_accuracy: 0.8822\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 2s 934us/step - loss: 0.2873 - accuracy: 0.8964 - val_loss: 0.3364 - val_accuracy: 0.8752\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 2s 952us/step - loss: 0.2809 - accuracy: 0.8976 - val_loss: 0.3158 - val_accuracy: 0.8852\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 2s 946us/step - loss: 0.2767 - accuracy: 0.9002 - val_loss: 0.3065 - val_accuracy: 0.8878\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 2s 945us/step - loss: 0.2704 - accuracy: 0.9030 - val_loss: 0.3128 - val_accuracy: 0.8852\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 2s 957us/step - loss: 0.2657 - accuracy: 0.9050 - val_loss: 0.3171 - val_accuracy: 0.8868\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 2s 943us/step - loss: 0.2607 - accuracy: 0.9067 - val_loss: 0.3071 - val_accuracy: 0.8902\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 2s 940us/step - loss: 0.2569 - accuracy: 0.9071 - val_loss: 0.3053 - val_accuracy: 0.8896\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 2s 942us/step - loss: 0.2515 - accuracy: 0.9091 - val_loss: 0.3051 - val_accuracy: 0.8880\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 2s 940us/step - loss: 0.2470 - accuracy: 0.9097 - val_loss: 0.3057 - val_accuracy: 0.8898\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 2s 953us/step - loss: 0.2433 - accuracy: 0.9122 - val_loss: 0.3149 - val_accuracy: 0.8854\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 2s 952us/step - loss: 0.2400 - accuracy: 0.9147 - val_loss: 0.3277 - val_accuracy: 0.8846\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 2s 946us/step - loss: 0.2354 - accuracy: 0.9147 - val_loss: 0.3171 - val_accuracy: 0.8882\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 2s 979us/step - loss: 0.2329 - accuracy: 0.9165 - val_loss: 0.2981 - val_accuracy: 0.8900\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 2s 962us/step - loss: 0.2284 - accuracy: 0.9179 - val_loss: 0.2938 - val_accuracy: 0.8926\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cd02ae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:41.476388Z",
     "start_time": "2022-04-18T04:28:41.319379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802e576",
   "metadata": {},
   "source": [
    "### 모델 평가 및 예측\n",
    "은닉층의 개수와 학습률 같은 하이퍼파라미터를 튜닝해가며 모델을 훈련시키고 마지막에 평가 할 때는  \n",
    "evaluate() 메서드를 이용하여 테스트셋을 평가하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "240e9e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:41.736409Z",
     "start_time": "2022-04-18T04:28:41.477341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 635us/step - loss: 0.3259 - accuracy: 0.8820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32585012912750244, 0.8820000290870667]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b851edf1",
   "metadata": {},
   "source": [
    "그 다음 제출할 파일을 제작하기 위하여 predict() 메서드를 이용하여 새로운 샘플에 대한 예측을 만들어 낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e24c5eb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:41.815406Z",
     "start_time": "2022-04-18T04:28:41.737390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.97],\n",
       "       [0.  , 0.  , 0.99, 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1edbbe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T06:03:27.999745Z",
     "start_time": "2022-04-15T06:03:27.979747Z"
    }
   },
   "source": [
    "predict만 놓고보면 softmax를 이용했으므로 확률이 출력된다.  \n",
    "만약 가장 높은 확률을 가진 클래스에만 관심이 있다면 predict_classes() 메서드를 이용할... 수 있었지만  \n",
    "사라졌다. 그러니 그냥 argmax를 이용해서 뽑아주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10cfe995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:41.861375Z",
     "start_time": "2022-04-18T04:28:41.816343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred = model.predict_classes(X_new) <- deprecated\n",
    "y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "print(y_pred)\n",
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9acba4",
   "metadata": {},
   "source": [
    "## 시퀸셜 API를 이용한 회귀 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44750b62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:42.209341Z",
     "start_time": "2022-04-18T04:28:41.862392Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d50a4302",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:47.247146Z",
     "start_time": "2022-04-18T04:28:42.210340Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.8736 - val_loss: 17.8288\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 649us/step - loss: 0.5267 - val_loss: 23.9346\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 648us/step - loss: 0.5855 - val_loss: 1.4132\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 619us/step - loss: 0.4250 - val_loss: 0.3848\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 635us/step - loss: 0.4014 - val_loss: 0.3834\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 654us/step - loss: 0.3918 - val_loss: 0.4302\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 638us/step - loss: 0.3842 - val_loss: 0.3935\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 626us/step - loss: 0.3785 - val_loss: 0.3848\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 627us/step - loss: 0.3728 - val_loss: 0.3642\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 619us/step - loss: 0.3683 - val_loss: 0.4224\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 641us/step - loss: 0.3667 - val_loss: 0.3636\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 649us/step - loss: 0.3632 - val_loss: 0.3788\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 619us/step - loss: 0.3602 - val_loss: 0.3495\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 624us/step - loss: 0.3595 - val_loss: 0.3500\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 635us/step - loss: 0.3570 - val_loss: 0.3706\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 710us/step - loss: 0.3544 - val_loss: 0.3505\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.3538 - val_loss: 0.3526\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 613us/step - loss: 0.3511 - val_loss: 0.3601\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 611us/step - loss: 0.3497 - val_loss: 0.3558\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 602us/step - loss: 0.3488 - val_loss: 0.3804\n",
      "162/162 [==============================] - 0s 423us/step - loss: 0.3459\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid,y_valid))\n",
    "mse_test = model.evaluate(X_test,y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b67b4",
   "metadata": {},
   "source": [
    "## 함수형 API를 사용해 복잡한 모델 생성하기\n",
    "지금까진 순차적으로 sequential을 이용하여 모델을 생성했지만 모델이 꼭 순차적일 필요는 없다.  \n",
    "신경망이 깊게 쌓일 수도 있지만 곧바로 깊은 층으로 건너 뛸수도 있다.  \n",
    "이렇게 구현하면 신경망이 복잡한 패턴과 단순한 규칙을 모두 학습할 수 있게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64290631",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:47.278167Z",
     "start_time": "2022-04-18T04:28:47.248096Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "# 여기까진 순차적이다만...\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "# input layer와 hidden2 layer를 바로 이어준다.\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1c3e6d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:47.294134Z",
     "start_time": "2022-04-18T04:28:47.279192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 30)           270         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 30)           930         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 38)           0           ['input_1[0][0]',                \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            39          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175903c5",
   "metadata": {},
   "source": [
    "### 여러 입력을 받고 출력하기\n",
    "또 일부 특성은 깊게, 나머지 특성을 얕게 보내버리고 싶다면 다음과 같이 하면된다.  \n",
    "이 코드에서는 5개의 특성을 곧바로 concat layer에 보내고 6개의 특성을 2개의 은닉층을 거치게 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9258694b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:47.326145Z",
     "start_time": "2022-04-18T04:28:47.295164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " deep_input (InputLayer)        [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 30)           210         ['deep_input[0][0]']             \n",
      "                                                                                                  \n",
      " wide_input (InputLayer)        [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 30)           930         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 35)           0           ['wide_input[0][0]',             \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            36          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,176\n",
      "Trainable params: 1,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dbceede",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:52.749154Z",
     "start_time": "2022-04-18T04:28:47.327096Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 826us/step - loss: 2.0609 - val_loss: 0.8371\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 665us/step - loss: 0.7124 - val_loss: 0.6309\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 681us/step - loss: 0.6199 - val_loss: 0.5695\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 688us/step - loss: 0.5736 - val_loss: 0.5331\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.5417 - val_loss: 0.5058\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 677us/step - loss: 0.5170 - val_loss: 0.4835\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.4971 - val_loss: 0.4639\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.4814 - val_loss: 0.4485\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.4688 - val_loss: 0.4377\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 677us/step - loss: 0.4587 - val_loss: 0.4291\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.4507 - val_loss: 0.4211\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.4442 - val_loss: 0.4140\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.4390 - val_loss: 0.4095\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.4345 - val_loss: 0.4043\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 677us/step - loss: 0.4307 - val_loss: 0.4011\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 726us/step - loss: 0.4276 - val_loss: 0.4034\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 669us/step - loss: 0.4247 - val_loss: 0.3970\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 699us/step - loss: 0.4222 - val_loss: 0.4003\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 760us/step - loss: 0.4201 - val_loss: 0.3912\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 732us/step - loss: 0.4180 - val_loss: 0.4023\n",
      "162/162 [==============================] - 0s 472us/step - loss: 0.4122\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6285d48d",
   "metadata": {},
   "source": [
    "### 다수의 출력을 만들어내기\n",
    "출력이 여러 개 필요한 상황이 많은데 이 경우에는 보조 출력을 만들어 주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15fa0c79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:52.781163Z",
     "start_time": "2022-04-18T04:28:52.750184Z"
    }
   },
   "outputs": [],
   "source": [
    "# 여기까진 위의 모델과 비슷해보인다.\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "\n",
    "# hidden2 layer에서 곧바로 보조 출력을 뽑아준다.\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])\n",
    "\n",
    "# 각 출력 층에 대한 손실함수와 최적화를 진행할 때 얼마나 가중치를 부여할 것인지를 정해주자.\n",
    "model.compile(loss=[\"mse\", \"mse\"],\n",
    "              loss_weights=[0.9, 0.1],\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b64fefa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:58.795319Z",
     "start_time": "2022-04-18T04:28:52.782178Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 987us/step - loss: 2.5271 - main_output_loss: 2.2613 - aux_output_loss: 4.9193 - val_loss: 2.2922 - val_main_output_loss: 2.0934 - val_aux_output_loss: 4.0809\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 763us/step - loss: 1.1042 - main_output_loss: 0.8357 - aux_output_loss: 3.5208 - val_loss: 1.0719 - val_main_output_loss: 0.8787 - val_aux_output_loss: 2.8108\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 760us/step - loss: 0.8612 - main_output_loss: 0.6713 - aux_output_loss: 2.5707 - val_loss: 0.8018 - val_main_output_loss: 0.6524 - val_aux_output_loss: 2.1462\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 771us/step - loss: 0.7531 - main_output_loss: 0.6106 - aux_output_loss: 2.0352 - val_loss: 0.6986 - val_main_output_loss: 0.5778 - val_aux_output_loss: 1.7853\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 757us/step - loss: 0.6886 - main_output_loss: 0.5732 - aux_output_loss: 1.7270 - val_loss: 0.6410 - val_main_output_loss: 0.5394 - val_aux_output_loss: 1.5548\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.6436 - main_output_loss: 0.5452 - aux_output_loss: 1.5292 - val_loss: 0.6046 - val_main_output_loss: 0.5163 - val_aux_output_loss: 1.4002\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 763us/step - loss: 0.6092 - main_output_loss: 0.5225 - aux_output_loss: 1.3893 - val_loss: 0.5788 - val_main_output_loss: 0.4990 - val_aux_output_loss: 1.2974\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 798us/step - loss: 0.5828 - main_output_loss: 0.5043 - aux_output_loss: 1.2890 - val_loss: 0.5523 - val_main_output_loss: 0.4776 - val_aux_output_loss: 1.2252\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 793us/step - loss: 0.5620 - main_output_loss: 0.4898 - aux_output_loss: 1.2119 - val_loss: 0.5315 - val_main_output_loss: 0.4596 - val_aux_output_loss: 1.1789\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 771us/step - loss: 0.5456 - main_output_loss: 0.4780 - aux_output_loss: 1.1540 - val_loss: 0.5179 - val_main_output_loss: 0.4483 - val_aux_output_loss: 1.1439\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 741us/step - loss: 0.5323 - main_output_loss: 0.4686 - aux_output_loss: 1.1060 - val_loss: 0.5070 - val_main_output_loss: 0.4391 - val_aux_output_loss: 1.1188\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 771us/step - loss: 0.5219 - main_output_loss: 0.4616 - aux_output_loss: 1.0649 - val_loss: 0.4988 - val_main_output_loss: 0.4320 - val_aux_output_loss: 1.1001\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 773us/step - loss: 0.5129 - main_output_loss: 0.4553 - aux_output_loss: 1.0315 - val_loss: 0.4925 - val_main_output_loss: 0.4271 - val_aux_output_loss: 1.0810\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 771us/step - loss: 0.5055 - main_output_loss: 0.4502 - aux_output_loss: 1.0032 - val_loss: 0.4886 - val_main_output_loss: 0.4248 - val_aux_output_loss: 1.0626\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 754us/step - loss: 0.4995 - main_output_loss: 0.4464 - aux_output_loss: 0.9776 - val_loss: 0.4910 - val_main_output_loss: 0.4287 - val_aux_output_loss: 1.0515\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 798us/step - loss: 0.4947 - main_output_loss: 0.4433 - aux_output_loss: 0.9581 - val_loss: 0.4804 - val_main_output_loss: 0.4183 - val_aux_output_loss: 1.0392\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 762us/step - loss: 0.4903 - main_output_loss: 0.4404 - aux_output_loss: 0.9394 - val_loss: 0.4798 - val_main_output_loss: 0.4195 - val_aux_output_loss: 1.0219\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 793us/step - loss: 0.4865 - main_output_loss: 0.4380 - aux_output_loss: 0.9229 - val_loss: 0.4733 - val_main_output_loss: 0.4141 - val_aux_output_loss: 1.0060\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 748us/step - loss: 0.4827 - main_output_loss: 0.4354 - aux_output_loss: 0.9086 - val_loss: 0.4733 - val_main_output_loss: 0.4162 - val_aux_output_loss: 0.9872\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 760us/step - loss: 0.4794 - main_output_loss: 0.4334 - aux_output_loss: 0.8938 - val_loss: 0.4692 - val_main_output_loss: 0.4132 - val_aux_output_loss: 0.9733\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train],\n",
    "                    epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9491a873",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:58.810419Z",
     "start_time": "2022-04-18T04:28:58.796337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'main_output_loss', 'aux_output_loss', 'val_loss', 'val_main_output_loss', 'val_aux_output_loss'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각각의 loss가 출력층 이름의 맞추어 저장된다.\n",
    "\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d75e9c5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:58.985670Z",
     "start_time": "2022-04-18T04:28:58.811359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 530us/step - loss: 0.4705 - main_output_loss: 0.4264 - aux_output_loss: 0.8671\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B],\n",
    "                                                 [y_test, y_test])\n",
    "\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7213cfcf",
   "metadata": {},
   "source": [
    "## 서브클래싱 API로 동적 모델 만들기\n",
    "미리 모델의 레이어의 크기와 연결 방식을 정의해야하는 정적 모델과 달리  \n",
    "반복문과 다양한 크기뿐만 아니라 조건문을 가지는 등, 동적인 모델을 짜야한다면 서브클래싱 API를 이용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "704b4308",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:28:59.001625Z",
     "start_time": "2022-04-18T04:28:58.987573Z"
    }
   },
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # 표준 매개변수를 처리한다.\n",
    "        \n",
    "        # units와 activation func을 지정해줄 수 있다.\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        \n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83ab78",
   "metadata": {},
   "source": [
    "## 모델 저장하고 복원하기\n",
    "모델을 HDF5 포맷을 이용하여 모델 뿐만 아니라 하이퍼파라미터와 파라미터까지도 저장할 수 있다.  \n",
    "또한 옵티마이저와 metrics까지도 저장되어 나중에 다시 꺼내어 쓸 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c9e7c06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:29:01.760827Z",
     "start_time": "2022-04-18T04:28:59.002618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 851us/step - loss: 1.7515 - val_loss: 0.8723\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 655us/step - loss: 0.6843 - val_loss: 0.6053\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.5995 - val_loss: 0.5547\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.5577 - val_loss: 0.5242\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.5262 - val_loss: 0.5105\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 658us/step - loss: 0.5017 - val_loss: 0.4710\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 655us/step - loss: 0.4819 - val_loss: 0.4476\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 652us/step - loss: 0.4669 - val_loss: 0.4487\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.4550 - val_loss: 0.4298\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 688us/step - loss: 0.4458 - val_loss: 0.4254\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "# 저장하기\n",
    "model.save(\"my_keras_model.h5\")\n",
    "\n",
    "# 가져오기\n",
    "get_model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680da1c2",
   "metadata": {},
   "source": [
    "## 콜백으로 체크포인트 저장하기\n",
    "fit() 메서드의 callbacks 파라미터를 사용하면 케라스가 훈련의 시작과 끝에 호출할 객체 리스트를 지정할 수 있다\n",
    "<br>\n",
    "기본적으로 하나의 에포크가 끝날 때 마다 호출된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44ffd75c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:29:15.861903Z",
     "start_time": "2022-04-18T04:29:01.761800Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.4770 - val_loss: 0.9570\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.6964 - val_loss: 0.6409\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.6270 - val_loss: 0.5796\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 688us/step - loss: 0.5804 - val_loss: 0.5583\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 707us/step - loss: 0.5435 - val_loss: 0.5342\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 718us/step - loss: 0.5132 - val_loss: 0.5097\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 646us/step - loss: 0.4891 - val_loss: 0.5093\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 702us/step - loss: 0.4692 - val_loss: 0.5039\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 699us/step - loss: 0.4533 - val_loss: 0.4892\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.4401 - val_loss: 0.4931\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.4306 - val_loss: 0.5200\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.4230 - val_loss: 0.4842\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.4160 - val_loss: 0.5155\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 657us/step - loss: 0.4107 - val_loss: 0.5101\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.4063 - val_loss: 0.5238\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.4025 - val_loss: 0.4887\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.3990 - val_loss: 0.4761\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 658us/step - loss: 0.3962 - val_loss: 0.4829\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.3933 - val_loss: 0.4568\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 658us/step - loss: 0.3908 - val_loss: 0.4957\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 644us/step - loss: 0.3884 - val_loss: 0.4629\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 655us/step - loss: 0.3863 - val_loss: 0.4577\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 644us/step - loss: 0.3841 - val_loss: 0.4777\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.3820 - val_loss: 0.4735\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.3799 - val_loss: 0.4889\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 638us/step - loss: 0.3786 - val_loss: 0.4859\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 644us/step - loss: 0.3769 - val_loss: 0.4742\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 691us/step - loss: 0.3754 - val_loss: 0.4505\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.3738 - val_loss: 0.4617\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.3723 - val_loss: 0.4391\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.3706 - val_loss: 0.4278\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.3693 - val_loss: 0.4447\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.3681 - val_loss: 0.4215\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 652us/step - loss: 0.3669 - val_loss: 0.4311\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.3657 - val_loss: 0.4233\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 649us/step - loss: 0.3645 - val_loss: 0.4249\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 681us/step - loss: 0.3636 - val_loss: 0.4131\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.3623 - val_loss: 0.4386\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 687us/step - loss: 0.3614 - val_loss: 0.4008\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 677us/step - loss: 0.3601 - val_loss: 0.4078\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.3590 - val_loss: 0.4011\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 649us/step - loss: 0.3583 - val_loss: 0.4194\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 638us/step - loss: 0.3573 - val_loss: 0.4071\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 725us/step - loss: 0.3564 - val_loss: 0.4006\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.3554 - val_loss: 0.3891\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 667us/step - loss: 0.3546 - val_loss: 0.4066\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 655us/step - loss: 0.3537 - val_loss: 0.4253\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.3531 - val_loss: 0.3909\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.3522 - val_loss: 0.3911\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 673us/step - loss: 0.3513 - val_loss: 0.3959\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.3503 - val_loss: 0.4060\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 656us/step - loss: 0.3501 - val_loss: 0.3923\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.3492 - val_loss: 0.4063\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.3482 - val_loss: 0.3994\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.3479 - val_loss: 0.3995\n",
      "162/162 [==============================] - 0s 512us/step - loss: 0.3562\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# 이러면 검증셋에서 가장 최고 점수를 기록한 모델이 저장된다.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\n",
    "                                                save_best_only=True)\n",
    "\n",
    "# 10번 기다렸는데도 정확도 진전이 없으면 종료시켜버린다.\n",
    "# restore_best_weights 를 True로 놓으면 최상의 metrics에서 가중치를 유지시킨다.\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "# 훈련이 Epoch 55에서 조기종료 됨을 볼 수 있다.\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")  # 최상의 모델로 롤백\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a69864",
   "metadata": {},
   "source": [
    "사용자 정의 콜백을 만드는 것도 가능하다.  \n",
    "https://www.tensorflow.org/guide/keras/custom_callback?hl=ko  \n",
    "여기 참고하는 게 더 빠를듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a9aee86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T04:29:16.711897Z",
     "start_time": "2022-04-18T04:29:15.862883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "286/363 [======================>.......] - ETA: 0s - loss: 0.3514\n",
      "val/train: 1.10\n",
      "363/363 [==============================] - 0s 721us/step - loss: 0.3547 - val_loss: 0.3912\n",
      "Epoch 2/3\n",
      "317/363 [=========================>....] - ETA: 0s - loss: 0.3518\n",
      "val/train: 1.19\n",
      "363/363 [==============================] - 0s 633us/step - loss: 0.3537 - val_loss: 0.4222\n",
      "Epoch 3/3\n",
      "318/363 [=========================>....] - ETA: 0s - loss: 0.3577\n",
      "val/train: 1.16\n",
      "363/363 [==============================] - 0s 638us/step - loss: 0.3529 - val_loss: 0.4089\n"
     ]
    }
   ],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "\n",
    "\n",
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=3,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b7812",
   "metadata": {},
   "source": [
    "## 텐서보드를 사용하여 시각화하기\n",
    "텐서플로를 설치하면 자동으로 설치되는 텐서보드를 활용하여 다차원 데이터를 시각화하고 통계를 확인하자.  \n",
    "텐서보드는 이벤트 파일이라는 이진 로그파일을 통해 자동으로 변경사항을 읽어 업데이트 해준다.  \n",
    "일반적으로 텐서보드 서버가 루트 로그 디렉터리를 가리키면 프로그램은 각각의 다른 서브디렉터리에 이벤트를 기록한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2acbb7ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T05:06:05.390752Z",
     "start_time": "2022-04-18T05:06:05.375723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\my_logs\\\\run_2022_04_18-14_06_05'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bd2cb45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T05:06:16.310790Z",
     "start_time": "2022-04-18T05:06:08.536106Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 881us/step - loss: 1.8866 - val_loss: 0.7126\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.6577 - val_loss: 0.6880\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.5934 - val_loss: 0.5803\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 669us/step - loss: 0.5557 - val_loss: 0.5166\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 655us/step - loss: 0.5272 - val_loss: 0.4895\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.5033 - val_loss: 0.4951\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.4854 - val_loss: 0.4861\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 702us/step - loss: 0.4709 - val_loss: 0.4554\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.4578 - val_loss: 0.4413\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.4474 - val_loss: 0.4379\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.4393 - val_loss: 0.4396\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 688us/step - loss: 0.4318 - val_loss: 0.4507\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 710us/step - loss: 0.4261 - val_loss: 0.3997\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 655us/step - loss: 0.4202 - val_loss: 0.3956\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.4155 - val_loss: 0.3916\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 738us/step - loss: 0.4112 - val_loss: 0.3937\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.4077 - val_loss: 0.3809\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.4040 - val_loss: 0.3793\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.4004 - val_loss: 0.3850\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.3980 - val_loss: 0.3809\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 702us/step - loss: 0.3949 - val_loss: 0.3701\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 707us/step - loss: 0.3924 - val_loss: 0.3781\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.3898 - val_loss: 0.3650\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.3874 - val_loss: 0.3655\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 669us/step - loss: 0.3851 - val_loss: 0.3611\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.3829 - val_loss: 0.3626\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.3809 - val_loss: 0.3564\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.3788 - val_loss: 0.3579\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.3769 - val_loss: 0.3561\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 646us/step - loss: 0.3750 - val_loss: 0.3548\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5453c",
   "metadata": {},
   "source": [
    "## 신경망 튜닝하기\n",
    "신경망이 유연해서 좋긴하다만 조정할 하이퍼파라미터 또한 유연하게 많다.  \n",
    "어떤 조합이 문제에 대해 최적인지 알려면 물론 bruteforce로 밀어버리면 되겠지만 GridSearch를 이용할 수 있다.  \n",
    "우선 신경망 모델을 사이킷런 추정기처럼 보이도록 만들어주고 GridSearch를 이용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e23833a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T05:30:46.549345Z",
     "start_time": "2022-04-18T05:30:46.527231Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2e0a6",
   "metadata": {},
   "source": [
    "KerasRegressor 객체는 build_model로 만들어진 케라스 모델을 감싸주는 wrapper이다.  \n",
    "이러면 이 케라스 모델을 마치 사이킷런의 추정기처럼 이용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8457c0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T06:36:40.709400Z",
     "start_time": "2022-04-18T06:36:32.809850Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/363 [..............................] - ETA: 45s - loss: 6.4368"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hesh0\\AppData\\Local\\Temp\\ipykernel_18548\\2686937247.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 838us/step - loss: 1.0333 - val_loss: 1.3945\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 757us/step - loss: 0.5285 - val_loss: 0.6407\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.4734 - val_loss: 0.5118\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 668us/step - loss: 0.4471 - val_loss: 0.4400\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.4320 - val_loss: 0.4250\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.4216 - val_loss: 0.4660\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 657us/step - loss: 0.4166 - val_loss: 0.4807\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.4110 - val_loss: 0.3846\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 669us/step - loss: 0.4065 - val_loss: 0.3838\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.4023 - val_loss: 0.4577\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.4005 - val_loss: 0.6151\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 657us/step - loss: 0.3982 - val_loss: 0.9757\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 655us/step - loss: 0.3999 - val_loss: 0.4965\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 655us/step - loss: 0.3925 - val_loss: 0.4422\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 657us/step - loss: 0.3903 - val_loss: 0.3721\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 624us/step - loss: 0.3879 - val_loss: 0.3624\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 646us/step - loss: 0.3857 - val_loss: 0.5129\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 652us/step - loss: 0.3862 - val_loss: 0.3938\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.3826 - val_loss: 0.4614\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 655us/step - loss: 0.3822 - val_loss: 0.3755\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 670us/step - loss: 0.3800 - val_loss: 0.3563\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 652us/step - loss: 0.3778 - val_loss: 0.6038\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 649us/step - loss: 0.3785 - val_loss: 1.1251\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 646us/step - loss: 0.3790 - val_loss: 0.9825\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 646us/step - loss: 0.3803 - val_loss: 0.4404\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 649us/step - loss: 0.3729 - val_loss: 0.7244\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.3753 - val_loss: 1.5421\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.3774 - val_loss: 1.4534\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.3780 - val_loss: 1.1109\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.3758 - val_loss: 1.5253\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.3821 - val_loss: 0.6606\n",
      "162/162 [==============================] - 0s 473us/step - loss: 0.3678\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023B6B974A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "keras_reg.fit(X_train,\n",
    "              y_train,\n",
    "              epochs=100,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d1f89",
   "metadata": {},
   "source": [
    "모델에 대한 여러 하이퍼파라미터를 조정해보자.  \n",
    "미리 build_model에서 지정해준 하이퍼파라미터들을 쪼개어 검사를 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "881759da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T06:43:16.213176Z",
     "start_time": "2022-04-18T06:39:04.533336Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 1.2963 - val_loss: 0.7823\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.7606 - val_loss: 0.6207\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.5484 - val_loss: 0.4616\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.4614 - val_loss: 0.4134\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4333 - val_loss: 0.4024\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4261 - val_loss: 0.4059\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4225 - val_loss: 0.3979\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4245 - val_loss: 0.4124\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.4333 - val_loss: 0.4090\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4198 - val_loss: 0.3886\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4164 - val_loss: 0.3959\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.4160 - val_loss: 0.4034\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4164 - val_loss: 0.3911\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4134 - val_loss: 0.3904\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4133 - val_loss: 0.3873\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4106 - val_loss: 0.3859\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4110 - val_loss: 0.3842\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4110 - val_loss: 0.3840\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4065 - val_loss: 0.3804\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4086 - val_loss: 0.3802\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4055 - val_loss: 0.3776\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4020 - val_loss: 0.3775\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3988 - val_loss: 0.3797\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4002 - val_loss: 0.4047\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3993 - val_loss: 0.3947\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3993 - val_loss: 0.3767\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3974 - val_loss: 0.3769\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4082 - val_loss: 0.3763\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3949 - val_loss: 0.3702\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3935 - val_loss: 0.3731\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.3942 - val_loss: 0.3695\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3923 - val_loss: 0.3669\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.3904 - val_loss: 0.3660\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3888 - val_loss: 0.3684\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3937 - val_loss: 0.3663\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3983 - val_loss: 0.3734\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.3892 - val_loss: 0.3649\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.3871 - val_loss: 0.3691\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3882 - val_loss: 0.3702\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3876 - val_loss: 0.3639\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3877 - val_loss: 0.3761\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4188 - val_loss: 0.3904\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.4215 - val_loss: 0.3727\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3919 - val_loss: 0.3675\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.3874 - val_loss: 0.3671\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.3885 - val_loss: 0.3715\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3854 - val_loss: 0.3676\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.3864 - val_loss: 0.3661\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.3896 - val_loss: 0.3649\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.3851 - val_loss: 0.3621\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3836 - val_loss: 0.4009\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3884 - val_loss: 0.3632\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3861 - val_loss: 0.3670\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3859 - val_loss: 0.3730\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3834 - val_loss: 0.3693\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3867 - val_loss: 0.3596\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3839 - val_loss: 0.3626\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.3857 - val_loss: 0.3628\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3839 - val_loss: 0.3661\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3808 - val_loss: 0.3707\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.3836 - val_loss: 0.3739\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3852 - val_loss: 0.4094\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.3845 - val_loss: 0.3680\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3822 - val_loss: 0.3678\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3896 - val_loss: 0.3762\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4048 - val_loss: 0.3713\n",
      "121/121 [==============================] - 0s 441us/step - loss: 0.4166\n",
      "[CV] END learning_rate=0.022174573948353458, n_hidden=1, n_neurons=4; total time=  11.9s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.8623 - val_loss: 6.3547\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5148 - val_loss: 5.4595\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4648 - val_loss: 2.8133\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4460 - val_loss: 1.5777\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4331 - val_loss: 0.9363\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4253 - val_loss: 0.5607\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.4199 - val_loss: 0.3893\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.4154 - val_loss: 0.3841\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.4109 - val_loss: 0.4297\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.4081 - val_loss: 0.3978\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.4044 - val_loss: 0.3938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4024 - val_loss: 0.3818\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.4020 - val_loss: 0.3733\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4006 - val_loss: 0.3751\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.3981 - val_loss: 0.3651\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.3954 - val_loss: 0.3942\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3954 - val_loss: 0.4503\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.3937 - val_loss: 0.5462\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.3914 - val_loss: 0.4705\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.3897 - val_loss: 0.6837\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3891 - val_loss: 1.7440\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.3879 - val_loss: 0.8276\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 643us/step - loss: 0.3861 - val_loss: 0.9196\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.3849 - val_loss: 0.4645\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.3852 - val_loss: 0.8943\n",
      "121/121 [==============================] - 0s 460us/step - loss: 0.4171\n",
      "[CV] END learning_rate=0.022174573948353458, n_hidden=1, n_neurons=4; total time=   4.4s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.7490 - val_loss: 7.2827\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4886 - val_loss: 0.7176\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4509 - val_loss: 0.3961\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4385 - val_loss: 0.3919\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4385 - val_loss: 0.3913\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.4330 - val_loss: 0.3978\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4336 - val_loss: 0.3869\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4310 - val_loss: 0.3882\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4294 - val_loss: 0.3836\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.4263 - val_loss: 0.3806\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4267 - val_loss: 0.3798\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4237 - val_loss: 0.3806\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4236 - val_loss: 0.3818\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4231 - val_loss: 0.3842\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.4223 - val_loss: 0.3808\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4189 - val_loss: 0.3749\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.4190 - val_loss: 0.3809\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4179 - val_loss: 0.3849\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4171 - val_loss: 0.3720\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4125 - val_loss: 0.3727\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.4116 - val_loss: 0.3663\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4103 - val_loss: 0.3692\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.4077 - val_loss: 0.3712\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.4062 - val_loss: 0.3663\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4184 - val_loss: 0.3638\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4035 - val_loss: 0.3668\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.4014 - val_loss: 0.3617\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.4122 - val_loss: 0.3658\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4010 - val_loss: 0.3621\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.4113 - val_loss: 0.3619\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3994 - val_loss: 0.3669\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4130 - val_loss: 0.3619\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3987 - val_loss: 0.3669\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4083 - val_loss: 0.3698\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4002 - val_loss: 0.3648\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3973 - val_loss: 0.3610\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.3981 - val_loss: 0.3752\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.4005 - val_loss: 0.3731\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3998 - val_loss: 0.3603\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3981 - val_loss: 0.3603\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.3958 - val_loss: 0.3620\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.3967 - val_loss: 0.3625\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.5171 - val_loss: 0.4231\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.4397 - val_loss: 0.3903\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.4210 - val_loss: 0.3793\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.4104 - val_loss: 0.3732\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.4036 - val_loss: 0.3629\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4005 - val_loss: 0.3611\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3966 - val_loss: 0.3653\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3976 - val_loss: 0.3627\n",
      "121/121 [==============================] - 0s 475us/step - loss: 0.3900\n",
      "[CV] END learning_rate=0.022174573948353458, n_hidden=1, n_neurons=4; total time=   9.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1681 - val_loss: 1.6515\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.5698 - val_loss: 8.3137\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.5256 - val_loss: 2.5326\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4502 - val_loss: 0.3893\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4006 - val_loss: 0.3719\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.3840 - val_loss: 0.3858\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3728 - val_loss: 0.3826\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3646 - val_loss: 0.4045\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3591 - val_loss: 0.3986\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3542 - val_loss: 0.3908\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3507 - val_loss: 0.3804\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.3464 - val_loss: 0.3608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.3424 - val_loss: 0.3848\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3420 - val_loss: 0.3991\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3380 - val_loss: 0.3725\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3365 - val_loss: 0.3804\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3338 - val_loss: 0.3800\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3322 - val_loss: 0.3798\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3303 - val_loss: 0.3781\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3283 - val_loss: 0.3683\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3278 - val_loss: 0.3894\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3257 - val_loss: 0.3733\n",
      "121/121 [==============================] - 0s 534us/step - loss: 0.3478\n",
      "[CV] END learning_rate=0.005432590230265343, n_hidden=2, n_neurons=94; total time=   4.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9613 - val_loss: 2.4274\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5509 - val_loss: 0.4711\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.4594 - val_loss: 0.4452\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4213 - val_loss: 0.3851\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3998 - val_loss: 0.3799\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3861 - val_loss: 0.5469\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3777 - val_loss: 0.7212\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3697 - val_loss: 0.8354\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3634 - val_loss: 1.0396\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3592 - val_loss: 0.7265\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3536 - val_loss: 0.8139\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3491 - val_loss: 0.9027\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.3461 - val_loss: 1.0453\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3438 - val_loss: 0.9002\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3412 - val_loss: 0.9043\n",
      "121/121 [==============================] - 0s 492us/step - loss: 0.3578\n",
      "[CV] END learning_rate=0.005432590230265343, n_hidden=2, n_neurons=94; total time=   3.3s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9282 - val_loss: 1.0605\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.5597 - val_loss: 4.5935\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.5014 - val_loss: 6.4904\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.5343 - val_loss: 4.0695\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4500 - val_loss: 0.4203\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4055 - val_loss: 0.4013\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3946 - val_loss: 0.3975\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3857 - val_loss: 0.3896\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3786 - val_loss: 0.4111\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3759 - val_loss: 0.3620\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3682 - val_loss: 0.3995\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3638 - val_loss: 0.3902\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.3596 - val_loss: 0.3387\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3571 - val_loss: 0.3660\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3525 - val_loss: 0.3587\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3485 - val_loss: 0.4018\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3471 - val_loss: 0.3287\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3438 - val_loss: 0.3518\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3411 - val_loss: 0.3657\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3382 - val_loss: 0.3633\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.3372 - val_loss: 0.3529\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3343 - val_loss: 0.3306\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3320 - val_loss: 0.3372\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3306 - val_loss: 0.4029\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3294 - val_loss: 0.3155\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3274 - val_loss: 0.3253\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3244 - val_loss: 0.3973\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3244 - val_loss: 0.3126\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3234 - val_loss: 0.3785\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3217 - val_loss: 0.3463\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3195 - val_loss: 0.3322\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3168 - val_loss: 0.3576\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3165 - val_loss: 0.4105\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3158 - val_loss: 0.3128\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3130 - val_loss: 0.3468\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3116 - val_loss: 0.3098\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3101 - val_loss: 0.3066\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.3100 - val_loss: 0.3775\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.3087 - val_loss: 0.3044\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3073 - val_loss: 0.4061\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3060 - val_loss: 0.3159\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3043 - val_loss: 0.4728\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3060 - val_loss: 0.3356\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3047 - val_loss: 0.3954\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3041 - val_loss: 0.3059\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3011 - val_loss: 0.3488\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.2994 - val_loss: 0.2939\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.2997 - val_loss: 0.3005\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.2969 - val_loss: 0.3552\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.2971 - val_loss: 0.3152\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.2955 - val_loss: 0.4377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.2960 - val_loss: 0.2998\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.2948 - val_loss: 0.2954\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2937 - val_loss: 0.3341\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.2931 - val_loss: 0.2900\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.2922 - val_loss: 0.5918\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.2947 - val_loss: 0.2990\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.2927 - val_loss: 0.4068\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.2918 - val_loss: 0.2904\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2898 - val_loss: 0.2896\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.2889 - val_loss: 0.2949\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.2872 - val_loss: 0.4200\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.2896 - val_loss: 0.4908\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.2903 - val_loss: 0.5036\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.2882 - val_loss: 0.3349\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.2874 - val_loss: 0.3997\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.2860 - val_loss: 0.2883\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2836 - val_loss: 0.5745\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2860 - val_loss: 0.4097\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2862 - val_loss: 0.4409\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.2854 - val_loss: 0.2828\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.2833 - val_loss: 0.3253\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.2842 - val_loss: 0.2997\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.2838 - val_loss: 0.7523\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.2833 - val_loss: 0.3068\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.2795 - val_loss: 1.3279\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.2910 - val_loss: 0.3939\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.2827 - val_loss: 0.4683\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.2828 - val_loss: 0.4543\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2812 - val_loss: 0.5302\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.2815 - val_loss: 0.4390\n",
      "121/121 [==============================] - 0s 475us/step - loss: 0.2977\n",
      "[CV] END learning_rate=0.005432590230265343, n_hidden=2, n_neurons=94; total time=  15.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 4.6620 - val_loss: 7.9302\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 2.5694 - val_loss: 4.4524\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 717us/step - loss: 1.6417 - val_loss: 2.3930\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 717us/step - loss: 1.1875 - val_loss: 1.3501\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.9555 - val_loss: 0.9131\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.8326 - val_loss: 0.7691\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.7653 - val_loss: 0.7179\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.7262 - val_loss: 0.6947\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.7015 - val_loss: 0.6681\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6837 - val_loss: 0.6623\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.6702 - val_loss: 0.6551\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.6592 - val_loss: 0.6318\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.6489 - val_loss: 0.6359\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.6401 - val_loss: 0.6313\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.6320 - val_loss: 0.6108\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.6239 - val_loss: 0.6099\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.6167 - val_loss: 0.6004\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.6094 - val_loss: 0.6011\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.6027 - val_loss: 0.5959\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5961 - val_loss: 0.5918\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5899 - val_loss: 0.5714\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.5835 - val_loss: 0.5719\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5777 - val_loss: 0.5666\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5719 - val_loss: 0.5641\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.5666 - val_loss: 0.5515\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.5612 - val_loss: 0.5386\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5559 - val_loss: 0.5440\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.5512 - val_loss: 0.5291\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.5464 - val_loss: 0.5274\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.5419 - val_loss: 0.5231\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.5375 - val_loss: 0.5191\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.5332 - val_loss: 0.5102\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.5291 - val_loss: 0.5071\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5251 - val_loss: 0.5020\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.5212 - val_loss: 0.4949\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5175 - val_loss: 0.4962\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5140 - val_loss: 0.4846\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5105 - val_loss: 0.4886\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5072 - val_loss: 0.4809\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5040 - val_loss: 0.4770\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.5009 - val_loss: 0.4787\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4980 - val_loss: 0.4712\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4951 - val_loss: 0.4674\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4922 - val_loss: 0.4676\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4896 - val_loss: 0.4659\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4870 - val_loss: 0.4618\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4845 - val_loss: 0.4601\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 734us/step - loss: 0.4821 - val_loss: 0.4573\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4797 - val_loss: 0.4553\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4774 - val_loss: 0.4554\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4753 - val_loss: 0.4519\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4732 - val_loss: 0.4472\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4711 - val_loss: 0.4446\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4690 - val_loss: 0.4424\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4670 - val_loss: 0.4421\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4652 - val_loss: 0.4377\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4633 - val_loss: 0.4382\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4615 - val_loss: 0.4350\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4598 - val_loss: 0.4326\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4580 - val_loss: 0.4299\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4564 - val_loss: 0.4289\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4548 - val_loss: 0.4281\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4532 - val_loss: 0.4268\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.4518 - val_loss: 0.4240\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4502 - val_loss: 0.4231\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4488 - val_loss: 0.4227\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.4474 - val_loss: 0.4209\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4461 - val_loss: 0.4179\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4447 - val_loss: 0.4173\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4434 - val_loss: 0.4164\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4422 - val_loss: 0.4154\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4410 - val_loss: 0.4138\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4398 - val_loss: 0.4135\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4386 - val_loss: 0.4120\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4375 - val_loss: 0.4103\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4364 - val_loss: 0.4098\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4354 - val_loss: 0.4081\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4343 - val_loss: 0.4072\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4333 - val_loss: 0.4063\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4323 - val_loss: 0.4052\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4313 - val_loss: 0.4045\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4303 - val_loss: 0.4034\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4294 - val_loss: 0.4026\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4284 - val_loss: 0.4018\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4276 - val_loss: 0.4007\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4266 - val_loss: 0.4000\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4258 - val_loss: 0.3992\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4249 - val_loss: 0.3984\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4241 - val_loss: 0.3977\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4233 - val_loss: 0.3969\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4224 - val_loss: 0.3963\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4217 - val_loss: 0.3955\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.4209 - val_loss: 0.3948\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4202 - val_loss: 0.3942\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4194 - val_loss: 0.3935\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4186 - val_loss: 0.3930\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4179 - val_loss: 0.3923\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.4172 - val_loss: 0.3916\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4165 - val_loss: 0.3916\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4159 - val_loss: 0.3906\n",
      "121/121 [==============================] - 0s 450us/step - loss: 0.4224\n",
      "[CV] END learning_rate=0.00037078874137762145, n_hidden=1, n_neurons=51; total time=  17.9s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 3.4433 - val_loss: 17.5871\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 1.8706 - val_loss: 22.0269\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 1.3330 - val_loss: 23.3104\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 1.1064 - val_loss: 22.6902\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.9892 - val_loss: 21.1362\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.9189 - val_loss: 19.3174\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.8724 - val_loss: 17.4679\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.8386 - val_loss: 15.6557\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.8120 - val_loss: 13.9994\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.7895 - val_loss: 12.4977\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.7698 - val_loss: 11.1582\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.7519 - val_loss: 9.8920\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.7356 - val_loss: 8.7937\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.7204 - val_loss: 7.8062\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.7061 - val_loss: 6.8945\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.6926 - val_loss: 6.1073\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.6797 - val_loss: 5.3874\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.6676 - val_loss: 4.7516\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.6560 - val_loss: 4.1878\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.6450 - val_loss: 3.6914\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.6346 - val_loss: 3.2396\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.6246 - val_loss: 2.8508\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.6151 - val_loss: 2.5047\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.6060 - val_loss: 2.2052\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 722us/step - loss: 0.5974 - val_loss: 1.9357\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.5891 - val_loss: 1.7025\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5811 - val_loss: 1.5026\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5736 - val_loss: 1.3241\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.5663 - val_loss: 1.1869\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5595 - val_loss: 1.0534\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.5529 - val_loss: 0.9362\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.5466 - val_loss: 0.8439\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.5406 - val_loss: 0.7632\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.5349 - val_loss: 0.7028\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5295 - val_loss: 0.6506\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.5243 - val_loss: 0.6022\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5193 - val_loss: 0.5661\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.5146 - val_loss: 0.5359\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5101 - val_loss: 0.5134\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5057 - val_loss: 0.4967\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.5015 - val_loss: 0.4843\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4977 - val_loss: 0.4746\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.4939 - val_loss: 0.4677\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.4903 - val_loss: 0.4640\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4868 - val_loss: 0.4623\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4835 - val_loss: 0.4625\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.4804 - val_loss: 0.4636\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4774 - val_loss: 0.4652\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4745 - val_loss: 0.4674\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4717 - val_loss: 0.4709\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4691 - val_loss: 0.4754\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4665 - val_loss: 0.4783\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4641 - val_loss: 0.4808\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4617 - val_loss: 0.4839\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4595 - val_loss: 0.4858\n",
      "121/121 [==============================] - 0s 458us/step - loss: 0.4659\n",
      "[CV] END learning_rate=0.00037078874137762145, n_hidden=1, n_neurons=51; total time=   9.8s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 3.7759 - val_loss: 10.5666\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 1.7938 - val_loss: 7.0846\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 1.0943 - val_loss: 3.7262\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.8296 - val_loss: 2.1577\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.7273 - val_loss: 1.3100\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.6820 - val_loss: 0.9445\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.6575 - val_loss: 0.7726\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.6417 - val_loss: 0.6768\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.6300 - val_loss: 0.6429\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.6204 - val_loss: 0.6201\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.6116 - val_loss: 0.6046\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.6036 - val_loss: 0.5931\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.5961 - val_loss: 0.5839\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.5889 - val_loss: 0.5763\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.5821 - val_loss: 0.5691\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.5757 - val_loss: 0.5612\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.5696 - val_loss: 0.5543\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5636 - val_loss: 0.5480\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.5580 - val_loss: 0.5415\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.5526 - val_loss: 0.5360\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.5473 - val_loss: 0.5309\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.5424 - val_loss: 0.5254\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5376 - val_loss: 0.5210\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5330 - val_loss: 0.5175\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5286 - val_loss: 0.5121\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5244 - val_loss: 0.5082\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5204 - val_loss: 0.5034\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.5165 - val_loss: 0.4994\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.5128 - val_loss: 0.4969\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.5093 - val_loss: 0.4929\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.5058 - val_loss: 0.4887\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.5025 - val_loss: 0.4858\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.4994 - val_loss: 0.4823\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4964 - val_loss: 0.4803\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4935 - val_loss: 0.4766\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4907 - val_loss: 0.4749\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4881 - val_loss: 0.4722\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4855 - val_loss: 0.4695\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4830 - val_loss: 0.4667\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4807 - val_loss: 0.4643\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4784 - val_loss: 0.4642\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4762 - val_loss: 0.4630\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4741 - val_loss: 0.4586\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4721 - val_loss: 0.4577\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4700 - val_loss: 0.4547\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.4681 - val_loss: 0.4527\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 708us/step - loss: 0.4663 - val_loss: 0.4516\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.4645 - val_loss: 0.4494\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.4628 - val_loss: 0.4496\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4611 - val_loss: 0.4487\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4594 - val_loss: 0.4472\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4579 - val_loss: 0.4466\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4564 - val_loss: 0.4447\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4549 - val_loss: 0.4430\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4534 - val_loss: 0.4423\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4520 - val_loss: 0.4431\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4507 - val_loss: 0.4410\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4493 - val_loss: 0.4404\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4480 - val_loss: 0.4391\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4467 - val_loss: 0.4349\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4455 - val_loss: 0.4363\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4442 - val_loss: 0.4388\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4431 - val_loss: 0.4355\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4419 - val_loss: 0.4325\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4408 - val_loss: 0.4319\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4396 - val_loss: 0.4346\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4386 - val_loss: 0.4292\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4375 - val_loss: 0.4303\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4365 - val_loss: 0.4296\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.4354 - val_loss: 0.4286\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.4344 - val_loss: 0.4250\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4334 - val_loss: 0.4251\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4324 - val_loss: 0.4234\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4315 - val_loss: 0.4245\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.4305 - val_loss: 0.4274\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.4296 - val_loss: 0.4284\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.4287 - val_loss: 0.4260\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.4279 - val_loss: 0.4224\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4271 - val_loss: 0.4222\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4262 - val_loss: 0.4254\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.4254 - val_loss: 0.4245\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.4246 - val_loss: 0.4234\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.4238 - val_loss: 0.4210\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4231 - val_loss: 0.4227\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4223 - val_loss: 0.4192\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4216 - val_loss: 0.4201\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4209 - val_loss: 0.4228\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.4202 - val_loss: 0.4203\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4195 - val_loss: 0.4229\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4189 - val_loss: 0.4194\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4182 - val_loss: 0.4181\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.4175 - val_loss: 0.4163\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4169 - val_loss: 0.4175\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.4162 - val_loss: 0.4180\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.4156 - val_loss: 0.4181\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4150 - val_loss: 0.4179\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4143 - val_loss: 0.4203\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.4137 - val_loss: 0.4157\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4132 - val_loss: 0.4164\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4126 - val_loss: 0.4182\n",
      "121/121 [==============================] - 0s 458us/step - loss: 0.4123\n",
      "[CV] END learning_rate=0.00037078874137762145, n_hidden=1, n_neurons=51; total time=  17.7s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1429 - val_loss: 2.1363\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.7205 - val_loss: 0.6521\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.6095 - val_loss: 0.5559\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.5637 - val_loss: 0.5157\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.5288 - val_loss: 0.4865\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.5006 - val_loss: 0.4677\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4777 - val_loss: 0.4439\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4587 - val_loss: 0.4383\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4435 - val_loss: 0.4352\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4313 - val_loss: 0.4291\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4213 - val_loss: 0.4188\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4129 - val_loss: 0.4080\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 717us/step - loss: 0.4061 - val_loss: 0.4153\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4003 - val_loss: 0.4354\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3954 - val_loss: 0.4252\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3911 - val_loss: 0.4260\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3872 - val_loss: 0.4340\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3838 - val_loss: 0.4232\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3800 - val_loss: 0.4316\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3777 - val_loss: 0.4198\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3749 - val_loss: 0.4320\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3724 - val_loss: 0.4122\n",
      "121/121 [==============================] - 0s 492us/step - loss: 0.3880\n",
      "[CV] END learning_rate=0.0016535051383872363, n_hidden=2, n_neurons=70; total time=   4.2s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 909us/step - loss: 1.7762 - val_loss: 4.8123\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.7222 - val_loss: 2.4570\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.6267 - val_loss: 1.4764\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.5750 - val_loss: 0.9755\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5353 - val_loss: 0.7328\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.5049 - val_loss: 0.6140\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4810 - val_loss: 0.5301\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4620 - val_loss: 0.4687\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4466 - val_loss: 0.4538\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.4351 - val_loss: 0.4301\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4249 - val_loss: 0.4205\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4165 - val_loss: 0.4088\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4102 - val_loss: 0.4053\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4046 - val_loss: 0.4249\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3996 - val_loss: 0.4238\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3952 - val_loss: 0.4414\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3909 - val_loss: 0.4540\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3880 - val_loss: 0.4808\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3840 - val_loss: 0.4836\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3816 - val_loss: 0.5108\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3794 - val_loss: 0.5408\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3767 - val_loss: 0.5694\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.3744 - val_loss: 0.6124\n",
      "121/121 [==============================] - 0s 441us/step - loss: 0.3838\n",
      "[CV] END learning_rate=0.0016535051383872363, n_hidden=2, n_neurons=70; total time=   4.3s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 2.0823 - val_loss: 2.1258\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.6857 - val_loss: 0.5970\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.6055 - val_loss: 0.5512\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.5628 - val_loss: 0.5114\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5265 - val_loss: 0.4947\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.4988 - val_loss: 0.4590\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4754 - val_loss: 0.4356\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4581 - val_loss: 0.4241\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4440 - val_loss: 0.4403\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4336 - val_loss: 0.3983\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4246 - val_loss: 0.4338\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4169 - val_loss: 0.4055\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4101 - val_loss: 0.3810\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4057 - val_loss: 0.4389\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4014 - val_loss: 0.3724\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3963 - val_loss: 0.4036\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3935 - val_loss: 0.3647\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3898 - val_loss: 0.3804\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3873 - val_loss: 0.3998\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3835 - val_loss: 0.3678\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.3814 - val_loss: 0.3716\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3793 - val_loss: 0.3534\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3764 - val_loss: 0.3623\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3736 - val_loss: 0.3939\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3720 - val_loss: 0.3466\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3693 - val_loss: 0.3491\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3673 - val_loss: 0.4026\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3668 - val_loss: 0.3514\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3661 - val_loss: 0.4325\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3652 - val_loss: 0.3496\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3610 - val_loss: 0.3467\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.3593 - val_loss: 0.3689\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3578 - val_loss: 0.3556\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3573 - val_loss: 0.4011\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3569 - val_loss: 0.3329\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3538 - val_loss: 0.3370\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3532 - val_loss: 0.3359\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3521 - val_loss: 0.4100\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3509 - val_loss: 0.3292\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3491 - val_loss: 0.3652\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3480 - val_loss: 0.3561\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3471 - val_loss: 0.3745\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.3463 - val_loss: 0.3372\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3451 - val_loss: 0.3573\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3446 - val_loss: 0.3285\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3430 - val_loss: 0.3483\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3418 - val_loss: 0.3334\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3415 - val_loss: 0.3210\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.3394 - val_loss: 0.3992\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3398 - val_loss: 0.3300\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3380 - val_loss: 0.3606\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3379 - val_loss: 0.3282\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3371 - val_loss: 0.3231\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3364 - val_loss: 0.3222\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 734us/step - loss: 0.3351 - val_loss: 0.3195\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3337 - val_loss: 0.3349\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3337 - val_loss: 0.3715\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3345 - val_loss: 0.3171\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3322 - val_loss: 0.3505\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3321 - val_loss: 0.3151\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3312 - val_loss: 0.3208\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3299 - val_loss: 0.3376\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3301 - val_loss: 0.3179\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3300 - val_loss: 0.3173\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.3287 - val_loss: 0.3458\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3281 - val_loss: 0.3731\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3279 - val_loss: 0.3338\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3267 - val_loss: 0.3499\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3262 - val_loss: 0.3536\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3256 - val_loss: 0.3820\n",
      "121/121 [==============================] - 0s 475us/step - loss: 0.3274\n",
      "[CV] END learning_rate=0.0016535051383872363, n_hidden=2, n_neurons=70; total time=  12.9s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 2.3002 - val_loss: 1451.2052\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 14.8864 - val_loss: 4492.2646\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 668us/step - loss: 34.6276 - val_loss: 23701.2188\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 672us/step - loss: 88.5954 - val_loss: 93347.2188\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 668us/step - loss: 379.2467 - val_loss: 430650.5000\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 16132.0137 - val_loss: 1879928.1250\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 19131.5684 - val_loss: 8436433.0000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 673us/step - loss: 282105.6562 - val_loss: 37757128.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 581983.2500 - val_loss: 170476560.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 7347071.5000 - val_loss: 833583040.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 12939395.0000 - val_loss: 3685265664.0000\n",
      "121/121 [==============================] - 0s 433us/step - loss: 9754721.0000\n",
      "[CV] END learning_rate=0.01824796188192035, n_hidden=0, n_neurons=40; total time=   2.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.9174 - val_loss: 22.6530\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.5162 - val_loss: 24.1832\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.5054 - val_loss: 25.5677\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5094 - val_loss: 22.8746\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.5092 - val_loss: 22.1608\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5087 - val_loss: 21.4923\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5111 - val_loss: 20.0412\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5100 - val_loss: 22.6030\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5068 - val_loss: 20.1552\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5086 - val_loss: 10.7312\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.5082 - val_loss: 19.7585\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5049 - val_loss: 24.3454\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.5077 - val_loss: 25.9642\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5199 - val_loss: 10.5321\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.5078 - val_loss: 17.1963\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5064 - val_loss: 21.8391\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5058 - val_loss: 11.7758\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.5101 - val_loss: 14.1567\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5071 - val_loss: 20.9828\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5032 - val_loss: 12.3627\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.5071 - val_loss: 25.9153\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.5132 - val_loss: 16.0464\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.5071 - val_loss: 19.4879\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5084 - val_loss: 12.1055\n",
      "121/121 [==============================] - 0s 453us/step - loss: 0.7813\n",
      "[CV] END learning_rate=0.01824796188192035, n_hidden=0, n_neurons=40; total time=   4.2s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 851us/step - loss: 1.2483 - val_loss: 271.0053\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.8537 - val_loss: 66.9167\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 1.3087 - val_loss: 716.3024\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 31.4885 - val_loss: 637.0506\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 1.2388 - val_loss: 2515.7410\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 8.6876 - val_loss: 1354.4408\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 4.5676 - val_loss: 1453.0856\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 660us/step - loss: 39.4905 - val_loss: 1322.2427\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 10.4597 - val_loss: 1268.9696\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 672us/step - loss: 1.4345 - val_loss: 194.2295\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 12.5373 - val_loss: 110.0689\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.6380 - val_loss: 0.9851\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.7371 - val_loss: 759.3539\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 8.8243 - val_loss: 447.5706\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 2.1803 - val_loss: 1044.4268\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 36.2997 - val_loss: 842.9007\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 8.7978 - val_loss: 1104.3945\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 3.7380 - val_loss: 485.6915\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 24.2129 - val_loss: 299.9521\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 3.5116 - val_loss: 340.7716\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 4.1854 - val_loss: 543.7151\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 4.4118 - val_loss: 381.9130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 0s 450us/step - loss: 0.6151\n",
      "[CV] END learning_rate=0.01824796188192035, n_hidden=0, n_neurons=40; total time=   4.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5226 - val_loss: 0.6269\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.5854 - val_loss: 0.5209\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5165 - val_loss: 0.4670\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4708 - val_loss: 0.4267\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.4397 - val_loss: 0.4009\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.4200 - val_loss: 0.3893\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4068 - val_loss: 0.3747\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3967 - val_loss: 0.3703\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3898 - val_loss: 0.3682\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3834 - val_loss: 0.3625\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3783 - val_loss: 0.3608\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3728 - val_loss: 0.3731\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3687 - val_loss: 0.3579\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3660 - val_loss: 0.3609\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3629 - val_loss: 0.3600\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3605 - val_loss: 0.3639\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3578 - val_loss: 0.3613\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3562 - val_loss: 0.3607\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3528 - val_loss: 0.3610\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.3513 - val_loss: 0.3590\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3494 - val_loss: 0.3653\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3479 - val_loss: 0.3627\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3454 - val_loss: 0.3573\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.3445 - val_loss: 0.3587\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3411 - val_loss: 0.3645\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3396 - val_loss: 0.3583\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3392 - val_loss: 0.3506\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3378 - val_loss: 0.3546\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3355 - val_loss: 0.3534\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3337 - val_loss: 0.3509\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3324 - val_loss: 0.3480\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3314 - val_loss: 0.3454\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3291 - val_loss: 0.3502\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3278 - val_loss: 0.3509\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3261 - val_loss: 0.3563\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3250 - val_loss: 0.3686\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3243 - val_loss: 0.3495\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3224 - val_loss: 0.3685\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3213 - val_loss: 0.3569\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3206 - val_loss: 0.3572\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3187 - val_loss: 0.3572\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3170 - val_loss: 0.3430\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3170 - val_loss: 0.3407\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3149 - val_loss: 0.3476\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3148 - val_loss: 0.3440\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3130 - val_loss: 0.3553\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3112 - val_loss: 0.3459\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3110 - val_loss: 0.3436\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3089 - val_loss: 0.3359\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3089 - val_loss: 0.3375\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3071 - val_loss: 0.3657\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3066 - val_loss: 0.3409\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3058 - val_loss: 0.3411\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3038 - val_loss: 0.3368\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3022 - val_loss: 0.3408\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3029 - val_loss: 0.3311\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3015 - val_loss: 0.3319\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3001 - val_loss: 0.3449\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.2997 - val_loss: 0.3280\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2982 - val_loss: 0.3305\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.2986 - val_loss: 0.3416\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.2972 - val_loss: 0.3467\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.2972 - val_loss: 0.3438\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.2959 - val_loss: 0.3485\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2941 - val_loss: 0.3368\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.2952 - val_loss: 0.3364\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.2939 - val_loss: 0.3487\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.2930 - val_loss: 0.3496\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.2923 - val_loss: 0.3310\n",
      "121/121 [==============================] - 0s 484us/step - loss: 0.3223\n",
      "[CV] END learning_rate=0.0045455096956331, n_hidden=3, n_neurons=30; total time=  13.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3089 - val_loss: 0.9467\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.5681 - val_loss: 1.4022\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4955 - val_loss: 1.0944\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4475 - val_loss: 0.6986\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4162 - val_loss: 0.4379\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.3971 - val_loss: 0.3688\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3858 - val_loss: 0.4551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3761 - val_loss: 0.5188\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3685 - val_loss: 0.5952\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3637 - val_loss: 0.5704\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3579 - val_loss: 0.5685\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3529 - val_loss: 0.6135\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3505 - val_loss: 0.6326\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3478 - val_loss: 0.6790\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3455 - val_loss: 0.6235\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3423 - val_loss: 0.6596\n",
      "121/121 [==============================] - 0s 491us/step - loss: 0.3573\n",
      "[CV] END learning_rate=0.0045455096956331, n_hidden=3, n_neurons=30; total time=   3.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1990 - val_loss: 1.1279\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.5711 - val_loss: 0.7104\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.4772 - val_loss: 0.4235\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4359 - val_loss: 0.5242\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4125 - val_loss: 0.3888\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3981 - val_loss: 0.4640\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3879 - val_loss: 0.3790\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3797 - val_loss: 0.4027\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3724 - val_loss: 0.4193\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3716 - val_loss: 0.3539\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3640 - val_loss: 0.4969\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3606 - val_loss: 0.3844\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3570 - val_loss: 0.3546\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3536 - val_loss: 0.5001\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3520 - val_loss: 0.3353\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3485 - val_loss: 0.4571\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3464 - val_loss: 0.3338\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3430 - val_loss: 0.3783\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3420 - val_loss: 0.3873\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3400 - val_loss: 0.3866\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3396 - val_loss: 0.3762\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3365 - val_loss: 0.3276\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3354 - val_loss: 0.3518\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3335 - val_loss: 0.4262\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3327 - val_loss: 0.3168\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3302 - val_loss: 0.3545\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3286 - val_loss: 0.4290\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3284 - val_loss: 0.3202\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3272 - val_loss: 0.4569\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3266 - val_loss: 0.3357\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3241 - val_loss: 0.3586\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.3228 - val_loss: 0.4017\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3221 - val_loss: 0.3744\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3205 - val_loss: 0.3646\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3194 - val_loss: 0.3160\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3180 - val_loss: 0.4037\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3182 - val_loss: 0.3259\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3166 - val_loss: 0.5753\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3169 - val_loss: 0.3184\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3152 - val_loss: 0.4450\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3132 - val_loss: 0.3360\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3123 - val_loss: 0.3889\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3120 - val_loss: 0.3028\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3101 - val_loss: 0.4024\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3109 - val_loss: 0.3073\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3080 - val_loss: 0.4083\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3075 - val_loss: 0.3069\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3092 - val_loss: 0.3284\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3056 - val_loss: 0.3836\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3054 - val_loss: 0.3542\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3037 - val_loss: 0.3942\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3047 - val_loss: 0.3148\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3030 - val_loss: 0.3075\n",
      "121/121 [==============================] - 0s 475us/step - loss: 0.3130\n",
      "[CV] END learning_rate=0.0045455096956331, n_hidden=3, n_neurons=30; total time=   9.9s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 2.1430 - val_loss: 27.2375\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 1.1346 - val_loss: 16.6629\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.8309 - val_loss: 1.3617\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.6283 - val_loss: 0.5682\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.5757 - val_loss: 0.5291\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5433 - val_loss: 0.5005\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.5165 - val_loss: 0.4811\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4946 - val_loss: 0.4611\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4758 - val_loss: 0.4590\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.4610 - val_loss: 0.4314\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4490 - val_loss: 0.4218\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.4395 - val_loss: 0.4235\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4314 - val_loss: 0.4109\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.4250 - val_loss: 0.4160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.4200 - val_loss: 0.4211\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.4154 - val_loss: 0.4068\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4116 - val_loss: 0.4193\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.4084 - val_loss: 0.4026\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.4049 - val_loss: 0.4188\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.4027 - val_loss: 0.4050\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4006 - val_loss: 0.4156\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.3980 - val_loss: 0.4036\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.3963 - val_loss: 0.4136\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.3944 - val_loss: 0.3896\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.3925 - val_loss: 0.4231\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3911 - val_loss: 0.3954\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3897 - val_loss: 0.4098\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3878 - val_loss: 0.4098\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3863 - val_loss: 0.4198\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3853 - val_loss: 0.3908\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3835 - val_loss: 0.3887\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3827 - val_loss: 0.3835\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.3812 - val_loss: 0.4058\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3802 - val_loss: 0.4073\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3793 - val_loss: 0.3768\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3778 - val_loss: 0.4111\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.3771 - val_loss: 0.3699\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.3759 - val_loss: 0.4173\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.3748 - val_loss: 0.3776\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.3744 - val_loss: 0.4005\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3733 - val_loss: 0.3900\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3720 - val_loss: 0.3757\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3720 - val_loss: 0.3744\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.3705 - val_loss: 0.4164\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.3703 - val_loss: 0.3755\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3687 - val_loss: 0.3836\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3682 - val_loss: 0.3997\n",
      "121/121 [==============================] - 0s 475us/step - loss: 0.3872\n",
      "[CV] END learning_rate=0.0020587676114196545, n_hidden=1, n_neurons=49; total time=   8.5s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 1.5747 - val_loss: 9.7352\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.8192 - val_loss: 4.3137\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.7278 - val_loss: 1.7854\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.6660 - val_loss: 0.7515\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.6164 - val_loss: 0.5722\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.5769 - val_loss: 0.6943\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.5450 - val_loss: 0.8956\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.5192 - val_loss: 1.0920\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4985 - val_loss: 1.1187\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4825 - val_loss: 1.0987\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4692 - val_loss: 1.0208\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4588 - val_loss: 0.9794\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4511 - val_loss: 0.8463\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4444 - val_loss: 0.6734\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.4388 - val_loss: 0.5839\n",
      "121/121 [==============================] - 0s 483us/step - loss: 0.4513\n",
      "[CV] END learning_rate=0.0020587676114196545, n_hidden=1, n_neurons=49; total time=   2.9s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 1.7419 - val_loss: 1.6751\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.8028 - val_loss: 1.0580\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.6937 - val_loss: 0.6591\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.6369 - val_loss: 0.5932\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.5915 - val_loss: 0.5910\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.5574 - val_loss: 0.5213\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.5308 - val_loss: 0.4969\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.5100 - val_loss: 0.5134\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.4940 - val_loss: 0.5065\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.4812 - val_loss: 0.4516\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4689 - val_loss: 0.4542\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4595 - val_loss: 0.4296\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4521 - val_loss: 0.4221\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4454 - val_loss: 0.4526\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4401 - val_loss: 0.4112\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4346 - val_loss: 0.4298\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.4303 - val_loss: 0.4073\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4266 - val_loss: 0.4001\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4231 - val_loss: 0.4075\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4199 - val_loss: 0.3946\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4171 - val_loss: 0.3917\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4147 - val_loss: 0.3874\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4123 - val_loss: 0.3854\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4096 - val_loss: 0.4388\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4085 - val_loss: 0.4083\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4059 - val_loss: 0.3814\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4038 - val_loss: 0.4272\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4031 - val_loss: 0.3856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4004 - val_loss: 0.4853\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.4006 - val_loss: 0.3730\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.3979 - val_loss: 0.3845\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.3963 - val_loss: 0.4059\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.3952 - val_loss: 0.3866\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.3936 - val_loss: 0.4195\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.3934 - val_loss: 0.3676\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.3910 - val_loss: 0.3776\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.3905 - val_loss: 0.3901\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3891 - val_loss: 0.5050\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3896 - val_loss: 0.3886\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.3869 - val_loss: 0.4254\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.3860 - val_loss: 0.3801\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.3847 - val_loss: 0.3919\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3838 - val_loss: 0.3824\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3830 - val_loss: 0.4276\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3826 - val_loss: 0.3591\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3809 - val_loss: 0.4088\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3802 - val_loss: 0.4119\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3801 - val_loss: 0.3713\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3783 - val_loss: 0.3891\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3778 - val_loss: 0.3832\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3767 - val_loss: 0.4055\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3766 - val_loss: 0.3693\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3759 - val_loss: 0.3540\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3749 - val_loss: 0.3518\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.3739 - val_loss: 0.3518\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.3732 - val_loss: 0.3858\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3731 - val_loss: 0.3696\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3724 - val_loss: 0.3771\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.3717 - val_loss: 0.3625\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 717us/step - loss: 0.3712 - val_loss: 0.3514\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.3705 - val_loss: 0.3556\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3696 - val_loss: 0.3894\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3698 - val_loss: 0.3709\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3688 - val_loss: 0.3709\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3684 - val_loss: 0.3937\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3676 - val_loss: 0.5319\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.3688 - val_loss: 0.5118\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3684 - val_loss: 0.7359\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3696 - val_loss: 0.7525\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.3710 - val_loss: 1.0137\n",
      "121/121 [==============================] - 0s 450us/step - loss: 0.3620\n",
      "[CV] END learning_rate=0.0020587676114196545, n_hidden=1, n_neurons=49; total time=  12.5s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1959 - val_loss: 1.3538\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.5771 - val_loss: 0.6821\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.4792 - val_loss: 0.4323\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.4278 - val_loss: 0.3956\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3997 - val_loss: 0.3742\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3829 - val_loss: 0.4232\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3724 - val_loss: 0.3665\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3634 - val_loss: 0.4308\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3580 - val_loss: 0.3859\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3532 - val_loss: 0.3959\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3496 - val_loss: 0.3655\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3445 - val_loss: 0.3580\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3407 - val_loss: 0.3942\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3390 - val_loss: 0.3822\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3349 - val_loss: 0.3622\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3329 - val_loss: 0.3883\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3308 - val_loss: 0.3381\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3286 - val_loss: 0.3735\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3258 - val_loss: 0.3558\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3242 - val_loss: 0.3549\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3230 - val_loss: 0.3354\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3208 - val_loss: 0.4087\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3200 - val_loss: 0.3163\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3181 - val_loss: 0.3231\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3142 - val_loss: 0.4240\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3155 - val_loss: 0.5162\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3174 - val_loss: 0.5126\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3138 - val_loss: 0.3189\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3097 - val_loss: 0.3799\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3077 - val_loss: 0.3094\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3063 - val_loss: 0.3159\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3046 - val_loss: 0.3012\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3023 - val_loss: 0.3447\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.3008 - val_loss: 0.3220\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.2994 - val_loss: 0.3025\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 801us/step - loss: 0.2983 - val_loss: 0.4089\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.2985 - val_loss: 0.5960\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3012 - val_loss: 0.5770\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.2988 - val_loss: 0.3268\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.2954 - val_loss: 0.4078\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.2942 - val_loss: 0.2976\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2914 - val_loss: 0.3161\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.2908 - val_loss: 0.2954\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.2887 - val_loss: 0.3436\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.2889 - val_loss: 0.2909\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.2869 - val_loss: 0.3295\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.2853 - val_loss: 0.3052\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.2853 - val_loss: 0.3024\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.2836 - val_loss: 0.3196\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.2836 - val_loss: 0.4166\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.2827 - val_loss: 0.3024\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2796 - val_loss: 0.3784\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2809 - val_loss: 0.2825\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.2779 - val_loss: 0.3196\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.2765 - val_loss: 0.2962\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.2768 - val_loss: 0.2918\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.2743 - val_loss: 0.3498\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.2750 - val_loss: 0.3186\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.2743 - val_loss: 0.3000\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.2722 - val_loss: 0.3560\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.2729 - val_loss: 0.2914\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.2704 - val_loss: 0.3443\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2707 - val_loss: 0.3034\n",
      "121/121 [==============================] - 0s 483us/step - loss: 0.3215\n",
      "[CV] END learning_rate=0.005803602934201024, n_hidden=3, n_neurons=74; total time=  12.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8996 - val_loss: 0.5990\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.5017 - val_loss: 0.5197\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.4248 - val_loss: 0.4184\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3910 - val_loss: 0.3596\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3726 - val_loss: 0.4284\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3615 - val_loss: 0.6298\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.3552 - val_loss: 0.9093\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3495 - val_loss: 1.0007\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3438 - val_loss: 1.1257\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3407 - val_loss: 0.8347\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3357 - val_loss: 0.8664\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.3321 - val_loss: 0.8589\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3298 - val_loss: 0.9186\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3281 - val_loss: 0.7753\n",
      "121/121 [==============================] - 0s 517us/step - loss: 0.3478\n",
      "[CV] END learning_rate=0.005803602934201024, n_hidden=3, n_neurons=74; total time=   2.9s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9170 - val_loss: 8.8056\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.5851 - val_loss: 2.0435\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.4603 - val_loss: 0.4170\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.4089 - val_loss: 0.4120\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3918 - val_loss: 0.3727\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3784 - val_loss: 0.3855\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.3715 - val_loss: 0.3529\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3646 - val_loss: 0.3519\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3580 - val_loss: 0.3959\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3565 - val_loss: 0.3339\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3507 - val_loss: 0.4118\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.3474 - val_loss: 0.3434\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3427 - val_loss: 0.3359\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3402 - val_loss: 0.3784\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3379 - val_loss: 0.3257\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3344 - val_loss: 0.3612\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3329 - val_loss: 0.3214\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3294 - val_loss: 0.3428\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3276 - val_loss: 0.3496\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3252 - val_loss: 0.3512\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3246 - val_loss: 0.3220\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3224 - val_loss: 0.3320\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3198 - val_loss: 0.3578\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3188 - val_loss: 0.3343\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.3161 - val_loss: 0.3151\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.3139 - val_loss: 0.3129\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.3107 - val_loss: 0.3459\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.3103 - val_loss: 0.3060\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3099 - val_loss: 0.4017\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3087 - val_loss: 0.3362\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.3059 - val_loss: 0.3041\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3035 - val_loss: 0.3560\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3036 - val_loss: 0.3187\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3010 - val_loss: 0.3077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3002 - val_loss: 0.3208\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.2971 - val_loss: 0.4304\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.2969 - val_loss: 0.3846\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.2984 - val_loss: 0.6250\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3018 - val_loss: 0.5091\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2982 - val_loss: 0.6354\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.2975 - val_loss: 0.3338\n",
      "121/121 [==============================] - 0s 458us/step - loss: 0.3108\n",
      "[CV] END learning_rate=0.005803602934201024, n_hidden=3, n_neurons=74; total time=   8.0s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.0262 - val_loss: 7.0553\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.6299 - val_loss: 2.8510\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.4880 - val_loss: 0.4025\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.4128 - val_loss: 0.3849\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.3894 - val_loss: 0.3770\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.3755 - val_loss: 0.3856\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3658 - val_loss: 0.3824\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.3573 - val_loss: 0.3949\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3516 - val_loss: 0.3940\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3471 - val_loss: 0.3874\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.3429 - val_loss: 0.3726\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3372 - val_loss: 0.3715\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3327 - val_loss: 0.3714\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3302 - val_loss: 0.3764\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3261 - val_loss: 0.3634\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3233 - val_loss: 0.3739\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3203 - val_loss: 0.3645\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3185 - val_loss: 0.3587\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3149 - val_loss: 0.3502\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3127 - val_loss: 0.3492\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.3110 - val_loss: 0.3614\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.3091 - val_loss: 0.3636\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3060 - val_loss: 0.3768\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3057 - val_loss: 0.3179\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3013 - val_loss: 0.3903\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.2997 - val_loss: 0.3159\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.2993 - val_loss: 0.3492\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.2980 - val_loss: 0.3693\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.2951 - val_loss: 0.3575\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.2928 - val_loss: 0.3018\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.2923 - val_loss: 0.3228\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.2910 - val_loss: 0.2980\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.2884 - val_loss: 0.3248\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.2869 - val_loss: 0.3642\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.2855 - val_loss: 0.2969\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.2844 - val_loss: 0.3699\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.2833 - val_loss: 0.2876\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.2814 - val_loss: 0.3982\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.2809 - val_loss: 0.2957\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.2793 - val_loss: 0.3677\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.2783 - val_loss: 0.2881\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2765 - val_loss: 0.3192\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.2759 - val_loss: 0.2930\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.2746 - val_loss: 0.3833\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.2745 - val_loss: 0.2849\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.2724 - val_loss: 0.3703\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.2711 - val_loss: 0.3539\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.2711 - val_loss: 0.2951\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.2693 - val_loss: 0.2875\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.2695 - val_loss: 0.3990\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.2683 - val_loss: 0.3057\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.2661 - val_loss: 0.3610\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.2664 - val_loss: 0.2851\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.2644 - val_loss: 0.3055\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.2632 - val_loss: 0.3417\n",
      "121/121 [==============================] - 0s 492us/step - loss: 0.3149\n",
      "[CV] END learning_rate=0.0059640580092043885, n_hidden=3, n_neurons=80; total time=  11.6s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8905 - val_loss: 1.2048\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.4807 - val_loss: 0.4126\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.4166 - val_loss: 0.3881\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3912 - val_loss: 0.4066\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3749 - val_loss: 0.4514\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3641 - val_loss: 0.6434\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3574 - val_loss: 0.8384\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3516 - val_loss: 0.9485\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.3454 - val_loss: 1.2112\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.3414 - val_loss: 0.8124\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3363 - val_loss: 0.9838\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3326 - val_loss: 0.8345\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3293 - val_loss: 1.0373\n",
      "121/121 [==============================] - 0s 467us/step - loss: 0.3484\n",
      "[CV] END learning_rate=0.0059640580092043885, n_hidden=3, n_neurons=80; total time=   3.1s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 0.9703 - val_loss: 3.9683\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.5143 - val_loss: 1.5779\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.4293 - val_loss: 0.3817\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3946 - val_loss: 0.4339\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3803 - val_loss: 0.3570\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3715 - val_loss: 0.3817\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.3641 - val_loss: 0.3509\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3579 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.3524 - val_loss: 0.3764\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.3492 - val_loss: 0.3270\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.3466 - val_loss: 0.3664\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3412 - val_loss: 0.3422\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.3366 - val_loss: 0.3275\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3354 - val_loss: 0.3631\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3323 - val_loss: 0.3214\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.3284 - val_loss: 0.3628\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3295 - val_loss: 0.3160\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.3266 - val_loss: 0.3360\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3242 - val_loss: 0.3508\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3208 - val_loss: 0.3512\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3194 - val_loss: 0.3219\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3176 - val_loss: 0.3043\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3153 - val_loss: 0.3388\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.3134 - val_loss: 0.3374\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3124 - val_loss: 0.3042\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3109 - val_loss: 0.3160\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3074 - val_loss: 0.3466\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3114 - val_loss: 0.3170\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3110 - val_loss: 0.3817\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3082 - val_loss: 0.3165\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3037 - val_loss: 0.3128\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3022 - val_loss: 0.3459\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3016 - val_loss: 0.3192\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.2990 - val_loss: 0.3123\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.2984 - val_loss: 0.3046\n",
      "121/121 [==============================] - 0s 467us/step - loss: 0.3124\n",
      "[CV] END learning_rate=0.0059640580092043885, n_hidden=3, n_neurons=80; total time=   7.1s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 1.1797 - val_loss: 1.9667\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.6021 - val_loss: 2.7629\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5187 - val_loss: 0.7596\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4551 - val_loss: 0.4500\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.4219 - val_loss: 0.3861\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.4047 - val_loss: 0.4704\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3934 - val_loss: 0.3749\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.3838 - val_loss: 0.4817\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3788 - val_loss: 0.3881\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3725 - val_loss: 0.4342\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3686 - val_loss: 0.3643\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3631 - val_loss: 0.3644\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.3591 - val_loss: 0.4226\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3573 - val_loss: 0.3992\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3533 - val_loss: 0.3752\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3510 - val_loss: 0.3976\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3480 - val_loss: 0.3989\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3460 - val_loss: 0.3668\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3425 - val_loss: 0.3891\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3415 - val_loss: 0.3728\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3396 - val_loss: 0.3670\n",
      "121/121 [==============================] - 0s 483us/step - loss: 0.3618\n",
      "[CV] END learning_rate=0.004591455636549438, n_hidden=2, n_neurons=59; total time=   4.3s\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9453 - val_loss: 0.7008\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.5398 - val_loss: 1.9304\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.4585 - val_loss: 2.0097\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.4239 - val_loss: 1.4591\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.4043 - val_loss: 0.9617\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3907 - val_loss: 0.4804\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3833 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3757 - val_loss: 0.3572\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3687 - val_loss: 0.4200\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3657 - val_loss: 0.3796\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3599 - val_loss: 0.4120\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 791us/step - loss: 0.3555 - val_loss: 0.4386\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3536 - val_loss: 0.4699\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3528 - val_loss: 0.4852\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3494 - val_loss: 0.4282\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.3466 - val_loss: 0.4625\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3449 - val_loss: 0.4159\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3433 - val_loss: 0.4636\n",
      "121/121 [==============================] - 0s 484us/step - loss: 0.3702\n",
      "[CV] END learning_rate=0.004591455636549438, n_hidden=2, n_neurons=59; total time=   3.6s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 967us/step - loss: 1.0569 - val_loss: 9.3686\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.6356 - val_loss: 4.2451\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.5184 - val_loss: 0.5671\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4507 - val_loss: 0.4594\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.4255 - val_loss: 0.4089\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.4123 - val_loss: 0.4453\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.4026 - val_loss: 0.3985\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3944 - val_loss: 0.3944\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3876 - val_loss: 0.4442\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3838 - val_loss: 0.3617\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3791 - val_loss: 0.4706\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3747 - val_loss: 0.3919\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3700 - val_loss: 0.3609\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3670 - val_loss: 0.4279\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3649 - val_loss: 0.3551\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3611 - val_loss: 0.3848\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3586 - val_loss: 0.3472\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3559 - val_loss: 0.3584\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3537 - val_loss: 0.3931\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3514 - val_loss: 0.3662\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3502 - val_loss: 0.3581\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3478 - val_loss: 0.3368\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3460 - val_loss: 0.3381\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3444 - val_loss: 0.3691\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.3434 - val_loss: 0.3268\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3409 - val_loss: 0.3362\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3390 - val_loss: 0.3975\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3388 - val_loss: 0.3256\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3403 - val_loss: 0.4120\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3366 - val_loss: 0.3341\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3334 - val_loss: 0.3450\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3322 - val_loss: 0.3558\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3321 - val_loss: 0.3329\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3288 - val_loss: 0.3733\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3319 - val_loss: 0.3426\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3271 - val_loss: 0.3755\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3272 - val_loss: 0.3203\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3260 - val_loss: 0.4585\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3265 - val_loss: 0.4007\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3250 - val_loss: 0.4266\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3227 - val_loss: 0.3203\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3209 - val_loss: 0.3616\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3214 - val_loss: 0.3613\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3199 - val_loss: 0.4186\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3215 - val_loss: 0.3380\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3175 - val_loss: 0.4271\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3168 - val_loss: 0.4186\n",
      "121/121 [==============================] - 0s 475us/step - loss: 0.3227\n",
      "[CV] END learning_rate=0.004591455636549438, n_hidden=2, n_neurons=59; total time=   8.7s\n",
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.8011 - val_loss: 1.2075\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.4591 - val_loss: 3.5161\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 688us/step - loss: 0.4298 - val_loss: 0.8890\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.3744 - val_loss: 0.7454\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.3636 - val_loss: 0.4868\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.3515 - val_loss: 0.4616\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 693us/step - loss: 0.3459 - val_loss: 0.3222\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 702us/step - loss: 0.3374 - val_loss: 0.3348\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 699us/step - loss: 0.3329 - val_loss: 0.3285\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 713us/step - loss: 0.3283 - val_loss: 0.4165\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.3279 - val_loss: 0.5752\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 690us/step - loss: 0.3291 - val_loss: 0.5397\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 691us/step - loss: 0.3224 - val_loss: 0.3042\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 691us/step - loss: 0.3179 - val_loss: 0.3054\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 702us/step - loss: 0.3149 - val_loss: 0.3088\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.3131 - val_loss: 0.3182\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 710us/step - loss: 0.3108 - val_loss: 0.3650\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 716us/step - loss: 0.3075 - val_loss: 0.3150\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 707us/step - loss: 0.3058 - val_loss: 0.2996\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 699us/step - loss: 0.3042 - val_loss: 0.3264\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 724us/step - loss: 0.3011 - val_loss: 0.2970\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.2989 - val_loss: 0.3291\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.2978 - val_loss: 0.3086\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.2965 - val_loss: 0.3445\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 718us/step - loss: 0.2947 - val_loss: 0.3244\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 707us/step - loss: 0.2927 - val_loss: 0.3337\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 702us/step - loss: 0.2915 - val_loss: 0.3748\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 718us/step - loss: 0.2918 - val_loss: 0.4178\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 691us/step - loss: 0.2900 - val_loss: 0.2938\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 705us/step - loss: 0.2877 - val_loss: 0.3441\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 716us/step - loss: 0.2881 - val_loss: 0.2858\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 707us/step - loss: 0.2842 - val_loss: 0.3073\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 713us/step - loss: 0.2834 - val_loss: 0.2803\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 693us/step - loss: 0.2813 - val_loss: 0.2845\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.2805 - val_loss: 0.3044\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 707us/step - loss: 0.2806 - val_loss: 0.2840\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 702us/step - loss: 0.2780 - val_loss: 0.3486\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 699us/step - loss: 0.2765 - val_loss: 0.3053\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.2794 - val_loss: 0.3561\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 688us/step - loss: 0.2762 - val_loss: 0.2869\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 693us/step - loss: 0.2756 - val_loss: 0.5646\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 688us/step - loss: 0.2782 - val_loss: 0.2983\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 691us/step - loss: 0.2718 - val_loss: 0.4007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x0000023B6B968E50>,\n",
       "                   param_distributions={'learning_rate': [0.001683454924600351,\n",
       "                                                          0.02390836445593178,\n",
       "                                                          0.008731907739399206,\n",
       "                                                          0.004725396149933917,\n",
       "                                                          0.0006154014789262348,\n",
       "                                                          0.0006153331256530192,\n",
       "                                                          0.0003920021771415983,\n",
       "                                                          0.01619845322936229,\n",
       "                                                          0.004779156784872302,\n",
       "                                                          0.007821074275112...\n",
       "                                                          0.005021425736625637,\n",
       "                                                          0.0005703073595961105,\n",
       "                                                          0.001151888789941251,\n",
       "                                                          0.001621231156394198,\n",
       "                                                          0.0024505367684280487,\n",
       "                                                          0.011155092541719619,\n",
       "                                                          0.0007524347058135697,\n",
       "                                                          0.0032032448128444043,\n",
       "                                                          0.004591455636549438,\n",
       "                                                          0.0003715541189658278, ...],\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, ...]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100).tolist(),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2).rvs(1000).tolist(),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg,\n",
    "                                   param_distribs,\n",
    "                                   n_iter=10,\n",
    "                                   cv=3,\n",
    "                                   verbose=2)\n",
    "rnd_search_cv.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=100,\n",
    "                  # k-fold 방식을 이용하기에 validation은 조기종료에만 이용된다.\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd592a4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T06:52:22.197851Z",
     "start_time": "2022-04-18T06:52:22.177824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neurons': 80, 'n_hidden': 3, 'learning_rate': 0.0059640580092043885}\n",
      "-0.3252355257670085\n"
     ]
    }
   ],
   "source": [
    "print(rnd_search_cv.best_params_)\n",
    "print(rnd_search_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b4920",
   "metadata": {},
   "source": [
    "### Evolutional autoML\n",
    "말은 이렇게 했지만 요즘은 최적의 하이퍼파라미터 뿐만 아니라 신경망 구조 자체를 진화론적 관점으로 접근한다.  \n",
    "https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html  \n",
    "요약하면 뉴런도 없는 하나의 입력층으로 시작해서 더 좋은 결과를 내는 모델을 선택해가며 신경망을 진화시킨다.  \n",
    "하이퍼파라미터는 당연하고 경사 하강법을 대체해서 개별 신경망을 만들어 냈다.  \n",
    "물론 이렇게 기술이 발전하더라도 블로그에서 언급했던 좋은 모델부터 시작하면 더 좋은 결과가 나오기에 미리 잘 고르자.\n",
    "<br>\n",
    "<br>\n",
    "### 은닉층 개수\n",
    "당연히도 복잡한 문제일수록 심층 신경망이 얕은 신경망보다 더 잘 작동한다.  \n",
    "현실의 문제가 계층적인 경우가 많으므로 신경망이 깊어질수록 계층적으로 작동하게 되고 좋은 솔루션으로 빨리 수렴할 수 있다.  \n",
    "물론 성능의 차이가 막 크지 않다고 알려져있지만 심층 신경망일수록 더 적은 뉴런으로 해결할 수 있다.  \n",
    "(얕은 경우 뉴런을 많이 써야하지만 층이 많다면 뉴런을 적게 써도 비슷한 성능이 나온다)  \n",
    "이런 방식을 이용하면 저수준 구조를 학습하지 않고도 다른 모델에서 고수준 구조만 따로 학습시킬 수 있다.  \n",
    "예를 들어 얼굴을 인식하는 저수준 구조 모델이 있다면 이를 이용하여 가중치, 편향을 그대로 두고 다음 층에서  \n",
    "헤어 스타일, 쌍커플등만 학습하면 되는데 이를 **전이학습**이라 부른다.\n",
    "<br>\n",
    "<br>\n",
    "### 은닉층의 뉴런 개수\n",
    "수 많은 저수준의 특성들은 비교적 적은 고수준의 특성으로 합쳐질 수 있다. 따라서 보통 입력층에서 뉴런을 크게 넣고  하위 층에서 점차 뉴런을 줄여나간다. (물론 같은 크기를 써도 동일하거나 더 나을 때가 있다)  \n",
    "층의 개수처럼 과대적합이 되기 전까지 뉴런의 개수또한 늘려가도 되지만 실제로는 처음부터 확 늘려두고  \n",
    "조기 종료나 규제 기법을 쓰는 것이 간단하고 효과적이다.  \n",
    "이는 구글에서 stretch pants라고 부르는데 바지가 크면 어찌어찌 입을 수 있는 것과 유사하기 때문이다.\n",
    "* ***일반적으로 은닉층의 뉴런 개수를 늘리기보단, 은닉층의 개수를 늘리는 편이 이득이 크다.***\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### 다른 하이퍼 파라미터\n",
    "* 학습률  \n",
    "보통 최적의 학습률은 훈련 알고리즘이 발산하는 학습률인 최대 학습률의 절반 정도 이다.\n",
    "<br>\n",
    "이 학습률을 찾기 위하여 매우 낮은 학습률 ($10^{-5}$)에서 시작해서 점진적으로 늘려가고\n",
    "<br>\n",
    "이를 수백번 반복하여 모델을 훈련시킨다. 반복적으로 일정한 값을 학습률에 곱해주면 학습률에 대한 그래프가 나오는데<br>**가장 저점이 아닌 10배 낮은 지점이 최적의 지점이다.**\n",
    "<br>\n",
    "<br>\n",
    "* 옵티마이저  \n",
    "당연하게도 구식 경사하강법보다 RMSProp이라던가 더 좋은 옵티마이저를 선택하고 고른<br>\n",
    "옵티마이저의 하이퍼파라미터를  튜닝하는 것이 중요하다.<br>\n",
    "<br>\n",
    "<br>\n",
    "* 배치 크기  \n",
    "배치가 클수록 GPU와 같은 병렬처리기를 효율적으로 활용할 수 있다. 실제로 많은 연구자들이 GPU 램에 맞는 가장 큰  배치를 이용할 것을 권고하지만 종종 훈련 초기에 불안정하게 훈련되는 경우가 발생할 수 있다.  \n",
    "그래서 그런지 배치를 적게 써야한다는 사람도 있고 이에 또 반대해서 학습률을 점차 늘려간다면  \n",
    "매우 큰 배치를 이용할 수 있다고 주장하는 사람도 있다.  \n",
    "확실히 뭐가 됬던 배치가 클수록 훈련 시간 자체는 짧아진다. 만약 훈련이 불안정하고 결과가 안좋다면 배치 크기를 줄이자.\n",
    "<br>\n",
    "<br>\n",
    "* 활성화 함수\n",
    "<br>\n",
    "<br>\n",
    "* 반복 횟수  \n",
    "그런데 조기 종료를 이용하는 경우가 다반사라 튜닝할 필요가 거의 없다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "310.417px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
